{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turned-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from agent import DQN\n",
    "from agent import train\n",
    "from replay import UniformReplayBuffer, PrioritizedReplayBuffer\n",
    "from replay import Transition\n",
    "from observer import AverageObserver, MaximumObserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-exemption",
   "metadata": {},
   "source": [
    "### settings and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mathematical-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the env\n",
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "# determine training episodes\n",
    "num_episodes = 1000\n",
    "\n",
    "# log interval\n",
    "log_interval = 50\n",
    "\n",
    "# max step per episode\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# batch size for sampling experience\n",
    "batch_size = 64\n",
    "\n",
    "# using PER or not\n",
    "use_prioritized_experience_buffer = True\n",
    "\n",
    "# buffer size\n",
    "replay_buffer_capacity = 10000\n",
    "\n",
    "# frequency for updating online network\n",
    "online_update_period = 1\n",
    "\n",
    "# if use soft update target network\n",
    "use_soft_update = False\n",
    "\n",
    "# update rate if using soft update\n",
    "target_update_tau = 1\n",
    "\n",
    "# frequency for synchronizing the target network\n",
    "target_sync_period = 1\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# discounting factor\n",
    "gamma = 1\n",
    "\n",
    "# epsilon-greedy\n",
    "epsilon = 0.1\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# loss function\n",
    "loss_function = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-naples",
   "metadata": {},
   "source": [
    "### ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informational-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of State Space -> (4,)\n",
      "Size of Action Space ->  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "# display\n",
    "print(\"shape of State Space -> {}\".format(\n",
    "    env.observation_space.shape\n",
    "))\n",
    "print(\"Size of Action Space ->  {}\".format(\n",
    "    env.action_space.n\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-council",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reflected-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent config\n",
    "config = {\n",
    "    \"type\": \"dqn\", # \"dqn\" or \"ddqn\", default is \"ddqn\"\n",
    "    \"network\": {\n",
    "        \"type\": \"dense\", # \"dense\", \"conv\", \"lstm\"\n",
    "        \"hidden_layers\": (32, 32),\n",
    "    },\n",
    "    \"gamma\": gamma,\n",
    "    \"epsilon\": epsilon,\n",
    "}\n",
    "\n",
    "# agent\n",
    "dqn = DQN(\n",
    "    config,\n",
    "    env.observation_space.shape,\n",
    "    env.action_space.n,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function\n",
    ")\n",
    "\n",
    "# replay buffer\n",
    "if use_prioritized_experience_buffer:\n",
    "    b_dqn = PrioritizedReplayBuffer(size=replay_buffer_capacity, alpha=0.6)\n",
    "else:\n",
    "    b_dqn = UniformReplayBuffer(size=replay_buffer_capacity)\n",
    "    \n",
    "# observer\n",
    "obv_dqn = [AverageObserver(10), MaximumObserver(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period: 50, average reward: 10.000, maximum reward: 11.000\n",
      "period: 100, average reward: 11.700, maximum reward: 16.000\n",
      "period: 150, average reward: 17.800, maximum reward: 28.000\n",
      "period: 200, average reward: 36.600, maximum reward: 61.000\n",
      "period: 250, average reward: 249.000, maximum reward: 429.000\n"
     ]
    }
   ],
   "source": [
    "dqn_rewards = train(\n",
    "    env, dqn, b_dqn, \n",
    "    num_episodes=num_episodes, \n",
    "    max_steps_per_episode=max_steps_per_episode,\n",
    "    batch_size=batch_size,\n",
    "    online_update_period=online_update_period,\n",
    "    target_sync_period=target_sync_period,\n",
    "    log_interval=log_interval,\n",
    "    use_soft_update=use_soft_update,\n",
    "    target_update_tau=target_update_tau,\n",
    "    observer=obv_dqn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
