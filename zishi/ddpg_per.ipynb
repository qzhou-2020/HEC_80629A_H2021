{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg_per_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otdpa1QZV19H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc08a750-ac44-4ae2-ce13-f4c44c6ea2f1"
      },
      "source": [
        "!pip install --upgrade --force-reinstall box2d-py\n",
        "!pip install --upgrade --force-reinstall gym[Box_2D]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 4.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 5.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting gym[Box_2D]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 5.5MB/s \n",
            "\u001b[33m  WARNING: gym 0.18.0 does not provide the extra 'box_2d'\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/91/ee427c42957f8c4cbe477bf4f8b7f608e003a17941e509d1777e58648cb3/scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 110kB/s \n",
            "\u001b[?25hCollecting numpy>=1.10.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 118kB/s \n",
            "\u001b[?25hCollecting pyglet<=1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.6MB/s \n",
            "\u001b[?25hCollecting Pillow<=7.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 35.7MB/s \n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Collecting future\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 37.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.18.0-cp37-none-any.whl size=1656452 sha256=075aff438780a83279d4c47c8db9c751016c2a10ff8912884163974f0ff5f1a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=7a6952bf74fcabbf332481466f0932c5e12627af51f44f4e28679da240a32650\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built gym future\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scipy, future, pyglet, Pillow, cloudpickle, gym\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 future-0.18.2 gym-0.18.0 numpy-1.20.1 pyglet-1.5.0 scipy-1.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpaMSP_LwgQp"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNXpnq0jt4uO"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,activations\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "RANDOM_SEEDS = 123\n",
        "\n",
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZi8mrsrt9HJ"
      },
      "source": [
        "# Ornstein-Uhlenbeck process\n",
        "class OUNoise:\n",
        "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = (self.x_prev \n",
        "            + self.theta * (self.mu - self.x_prev) * self.dt \n",
        "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape))\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x_initial if self.x_initial is not None else np.zeros_like(self.mu)\n",
        "\n",
        "class SumTree(object):\n",
        "    \"\"\"\n",
        "    This SumTree code is a modified version and the original code is from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
        "    Story data with its priority in the tree.\n",
        "    \"\"\"\n",
        "    data_pointer = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity  # for all priority values\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
        "        #             size: capacity - 1                       size: capacity\n",
        "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
        "        # [--------------data frame-------------]\n",
        "        #             size: capacity\n",
        "\n",
        "    def add(self, p, data):\n",
        "        tree_idx = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data  # update data_frame\n",
        "        self.update(tree_idx, p)  # update tree_frame\n",
        "\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_idx, p):\n",
        "        change = p - self.tree[tree_idx]\n",
        "        self.tree[tree_idx] = p\n",
        "        # then propagate the change through tree\n",
        "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
        "            tree_idx = (tree_idx - 1) // 2\n",
        "            self.tree[tree_idx] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"\n",
        "        Tree structure and array storage:\n",
        "        Tree index:\n",
        "             0         -> storing priority sum\n",
        "            / \\\n",
        "          1     2\n",
        "         / \\   / \\\n",
        "        3   4 5   6    -> storing priority for transitions\n",
        "        Array type for storing:\n",
        "        [0,1,2,3,4,5,6]\n",
        "        \"\"\"\n",
        "        parent_idx = 0\n",
        "        while True:     # the while loop is faster than the method in the reference code\n",
        "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
        "            cr_idx = cl_idx + 1\n",
        "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
        "                leaf_idx = parent_idx\n",
        "                break\n",
        "            else:       # downward search, always search for a higher priority node\n",
        "                if v <= self.tree[cl_idx]:\n",
        "                    parent_idx = cl_idx\n",
        "                else:\n",
        "                    v -= self.tree[cl_idx]\n",
        "                    parent_idx = cr_idx\n",
        "\n",
        "        data_idx = leaf_idx - self.capacity + 1\n",
        "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
        "\n",
        "    @property\n",
        "    def total_p(self):\n",
        "        return self.tree[0]  # the root\n",
        "\n",
        "\n",
        "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    \"\"\"\n",
        "    This Memory class is modified based on the original code from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
        "    \"\"\"\n",
        "    epsilon = 0.01  # small amount to avoid zero priority\n",
        "    alpha = 0.7  # [0~1] convert the importance of TD error to priority\n",
        "    beta = 0.5  # importance-sampling, from initial value increasing to 1\n",
        "    beta_increment_per_sampling = 0.001\n",
        "    abs_err_upper = 1.  # clipped abs error\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.sample_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sample_count\n",
        "\n",
        "    def store(self, transition):\n",
        "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
        "        if max_p == 0:\n",
        "            max_p = self.abs_err_upper\n",
        "        self.tree.add(max_p, transition)   # set the max p for new p\n",
        "        self.sample_count = min(self.sample_count + 1, self.tree.capacity)\n",
        "\n",
        "    def sample(self, n):\n",
        "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
        "        pri_seg = self.tree.total_p / n       # priority segment\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
        "\n",
        "        a = self.tree.tree[-self.tree.capacity:]\n",
        "        min_prob = np.min(a[a != 0]) / self.tree.total_p     # for later calculate ISweight\n",
        "        for i in range(n):\n",
        "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
        "            v = np.random.uniform(a, b)\n",
        "            idx, p, data = self.tree.get_leaf(v)\n",
        "            prob = p / self.tree.total_p\n",
        "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
        "            b_idx[i], b_memory[i, :] = idx, data\n",
        "\n",
        "        return b_idx, b_memory, ISWeights\n",
        "\n",
        "    def batch_update(self, tree_idx, abs_errors):\n",
        "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
        "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
        "        ps = np.power(clipped_errors, self.alpha)\n",
        "        for ti, p in zip(tree_idx, ps):\n",
        "            self.tree.update(ti, p)\n",
        "\n",
        "\n",
        "def get_actor(state_shape, action_dim, upper_bound, units=(512,512)):\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.004, maxval=0.004)\n",
        "    inputs = layers.Input(shape=(state_shape,))\n",
        "    for idx, unit in enumerate(units):\n",
        "        if idx == 0:\n",
        "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(inputs)\n",
        "        else:\n",
        "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(action_dim, name=\"Output\", activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "    scaled_outputs = outputs * upper_bound\n",
        "    model = Model(inputs,scaled_outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic(state_shape, action_dim, state_units=(512,256),action_units=128,concat_units=128):\n",
        "    state_input = layers.Input(shape=(state_shape), name=\"state_input\")\n",
        "    for idx, unit in enumerate(state_units):      \n",
        "        if idx == 0:\n",
        "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_input)\n",
        "        else:\n",
        "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_out)\n",
        "    \n",
        "    action_input = layers.Input(shape=(action_dim),name=\"action_input\")\n",
        "    action_out = layers.Dense(action_units, name=\"action_lvl\", activation=\"relu\")(action_input)\n",
        "    \n",
        "    concat = layers.Concatenate()([state_out,action_out])\n",
        "    out = layers.Dense(concat_units, name=\"concat_lvl\",activation=\"relu\")(concat)\n",
        "    outputs = layers.Dense(1)(out) # Q value\n",
        "\n",
        "    model = Model([state_input, action_input], outputs)\n",
        "    return model\n",
        "\n",
        "@tf.function\n",
        "def update_target(model, target_model, tau=0.001):\n",
        "    weights = model.variables\n",
        "    target_weights = target_model.variables\n",
        "    for (a,b) in zip(target_weights,weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP_5PWKuhj6T"
      },
      "source": [
        "class DDPG:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 per=False,\n",
        "                 actor_lr=0.00005,\n",
        "                 critic_lr=0.0005,\n",
        "                 actor_units=(512,512),\n",
        "                 state_units=(512,512),\n",
        "                 action_units=128,\n",
        "                 concat_units=256,\n",
        "                 noise=\"OU\",\n",
        "                 tau=0.001,\n",
        "                 gamma=0.99,\n",
        "                 batch_size=256,\n",
        "                 memory_size=2**20\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.per = per\n",
        "        self.state_shape = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.upper_bound = env.action_space.high[0]\n",
        "        self.lower_bound = env.action_space.low[0]\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.epsilon_min = 0.001\n",
        "        if noise == \"OU\":\n",
        "            self.noise = OUNoise(mu=np.zeros(self.action_dim),sigma=float(0.2)*np.ones(self.action_dim))\n",
        "        else:\n",
        "            self.noise = np.random.normal(scale=0.2, size=self.action_dim)\n",
        "        self.actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
        "        self.critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
        "        self.target_actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
        "        self.target_critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
        "        self.target_actor_model.set_weights(self.actor_model.get_weights())\n",
        "        self.target_critic_model.set_weights(self.critic_model.get_weights())\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = Memory(capacity=memory_size) if per else deque(maxlen=memory_size)\n",
        "\n",
        "    def policy(self, state, noise_object):\n",
        "        sampled_actions = tf.squeeze(self.actor_model(state))\n",
        "        if np.random.rand() < max(self.epsilon,self.epsilon_min):\n",
        "            noise = noise_object()\n",
        "            sampled_actions = sampled_actions.numpy() + noise\n",
        "        else:\n",
        "            sampled_actions = sampled_actions.numpy()\n",
        "        legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
        "        return np.squeeze(legal_action)\n",
        "\n",
        "    def record(self, obs_array):\n",
        "        if self.per:\n",
        "            transition = np.hstack(obs_array)\n",
        "            self.memory.store(transition)\n",
        "        else:\n",
        "            self.memory.append(obs_array)    \n",
        "\n",
        "    def per_update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights, tree_idx\n",
        "    ):\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
        "            y = reward_batch + self.gamma * self.target_critic_model(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            ) * (1. - done_batch)\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            errors = y - critic_value\n",
        "            critic_loss = tf.math.reduce_mean(ISWeights * tf.math.square(errors))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, self.critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        abs_errors = tf.reduce_sum(tf.abs(errors), axis=1)\n",
        "        self.memory.batch_update(tree_idx, abs_errors)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "    ):\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
        "            y = reward_batch + self.gamma * self.target_critic_model(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            ) * (1. - done_batch)\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.keras.losses.Huber()(y,critic_value)\n",
        "\n",
        "            critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "            self.critic_optimizer.apply_gradients(\n",
        "                zip(critic_grad, self.critic_model.trainable_variables)\n",
        "            )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        if self.per:\n",
        "            tree_idx, samples, ISWeights = self.memory.sample(min(self.batch_size,len(self.memory)))\n",
        "            split_shape = np.cumsum([self.state_shape, self.action_dim, 1, self.state_shape])\n",
        "            states, actions, rewards, next_states, dones = np.hsplit(samples, split_shape)\n",
        "            state_batch = tf.convert_to_tensor(states)\n",
        "            action_batch = tf.convert_to_tensor(actions)\n",
        "            reward_batch = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(next_states)\n",
        "            done_batch = tf.convert_to_tensor(dones,dtype=tf.float32)\n",
        "            self.per_update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights, tree_idx)\n",
        "        else:\n",
        "            samples = random.sample(self.memory, min(self.batch_size,len(self.memory)))\n",
        "            trans_s = np.array(samples,dtype=object).T\n",
        "            state_batch = tf.convert_to_tensor(np.row_stack(trans_s[0]))\n",
        "            action_batch = tf.convert_to_tensor(np.row_stack(trans_s[1]))\n",
        "            reward_batch = tf.convert_to_tensor(np.row_stack(trans_s[2]),dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(np.row_stack(trans_s[3]))\n",
        "            done_batch = tf.convert_to_tensor(np.row_stack(trans_s[4]),dtype=tf.float32)\n",
        "            self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "    def train(self, max_episodes=2000, max_steps=1000):\n",
        "        self.ep_reward_list = []\n",
        "        self.avg_reward_list = []\n",
        "        self.ep_steps_list = []\n",
        "\n",
        "        for ep in range(max_episodes):\n",
        "            prev_state = self.env.reset()\n",
        "            episodic_reward = 0\n",
        "            steps = 0\n",
        "            \n",
        "            while steps < max_steps:\n",
        "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "                action = self.policy(tf_prev_state, self.noise)\n",
        "\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                self.record([prev_state, action, reward, state, done])\n",
        "\n",
        "                episodic_reward += reward\n",
        "                self.replay()\n",
        "\n",
        "                update_target(self.actor_model, self.target_actor_model, self.tau)\n",
        "                update_target(self.critic_model, self.target_critic_model, self.tau)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                prev_state = state\n",
        "                steps += 1\n",
        "            self.epsilon*=self.epsilon_decay\n",
        "            self.ep_reward_list.append(episodic_reward)\n",
        "            # Mean of last 100 episodes\n",
        "            avg_reward = np.mean(self.ep_reward_list[-100:])\n",
        "            if ep%10 == 0:\n",
        "                print(f\"Episode * {ep} * Steps {steps}  * epsilon {self.epsilon} * Episodic Reward is ==> {episodic_reward} * Lastest 100 Episods Avg Reward is ==> {avg_reward}\")\n",
        "            self.avg_reward_list.append(avg_reward)\n",
        "            self.ep_steps_list.append(steps)\n",
        "\n",
        "        plt.plot(self.avg_reward_list)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "        plt.show()\n",
        "\n",
        "        content = {\"ep_reward\":ep_reward_list,\"avg_reward\":avg_reward_list, \"steps\":ep_steps_list}\n",
        "        df = pd.DataFrame(content)\n",
        "        df.to_csv(f'ddpg_per_{self.per}_{date_time}.csv') \n",
        "        files.download(f'ddpg_per_{self.per}_{date_time}.csv')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmmC-HiYh16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174814d8-33fe-4434-9d95-00b9bfeeb066"
      },
      "source": [
        "problem = 'LunarLanderContinuous-v2'\n",
        "gym_env = gym.make(problem)\n",
        "gym_env.seed(RANDOM_SEEDS)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkJwfK0hvvw"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "ddpg = DDPG(gym_env,per=False)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb2tBUrvjynI",
        "outputId": "ed8b3fca-3223-422c-8a8c-bc4120c5624a"
      },
      "source": [
        "ddpg.train(max_episodes=2000,max_steps=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Steps 229  * epsilon 0.999 * Episodic Reward is ==> -477.9026641369475 * Lastest 100 Episods Avg Reward is ==> -477.9026641369475\n",
            "Episode * 10 * Steps 115  * epsilon 0.9890548353295385 * Episodic Reward is ==> -324.2536901890662 * Lastest 100 Episods Avg Reward is ==> -370.6092527351145\n",
            "Episode * 20 * Steps 119  * epsilon 0.9792086759647052 * Episodic Reward is ==> -540.2468650923074 * Lastest 100 Episods Avg Reward is ==> -350.0380967420201\n",
            "Episode * 30 * Steps 170  * epsilon 0.9694605362958227 * Episodic Reward is ==> -114.52999090529694 * Lastest 100 Episods Avg Reward is ==> -307.8534497970426\n",
            "Episode * 40 * Steps 102  * epsilon 0.959809440525076 * Episodic Reward is ==> -917.63372040522 * Lastest 100 Episods Avg Reward is ==> -364.3945207513899\n",
            "Episode * 50 * Steps 108  * epsilon 0.9502544225688344 * Episodic Reward is ==> -320.9741886910264 * Lastest 100 Episods Avg Reward is ==> -384.3669371960227\n",
            "Episode * 60 * Steps 326  * epsilon 0.9407945259609451 * Episodic Reward is ==> 1.1757499543040666 * Lastest 100 Episods Avg Reward is ==> -372.7210235590801\n",
            "Episode * 70 * Steps 108  * epsilon 0.9314288037569908 * Episodic Reward is ==> -369.73807962825856 * Lastest 100 Episods Avg Reward is ==> -374.5618418760092\n",
            "Episode * 80 * Steps 93  * epsilon 0.9221563184394991 * Episodic Reward is ==> -191.2339124123201 * Lastest 100 Episods Avg Reward is ==> -347.707013425321\n",
            "Episode * 90 * Steps 99  * epsilon 0.9129761418240965 * Episodic Reward is ==> -38.30501212409055 * Lastest 100 Episods Avg Reward is ==> -330.72211185338875\n",
            "Episode * 100 * Steps 999  * epsilon 0.9038873549665959 * Episodic Reward is ==> -118.42718468538027 * Lastest 100 Episods Avg Reward is ==> -320.5283091405847\n",
            "Episode * 110 * Steps 150  * epsilon 0.8948890480710096 * Episodic Reward is ==> -401.8717192820947 * Lastest 100 Episods Avg Reward is ==> -308.9458129196958\n",
            "Episode * 120 * Steps 114  * epsilon 0.8859803203984784 * Episodic Reward is ==> -26.741115336318146 * Lastest 100 Episods Avg Reward is ==> -294.5802326755685\n",
            "Episode * 130 * Steps 93  * epsilon 0.8771602801771059 * Episodic Reward is ==> -102.99699177033013 * Lastest 100 Episods Avg Reward is ==> -276.2135197734083\n",
            "Episode * 140 * Steps 999  * epsilon 0.8684280445126921 * Episodic Reward is ==> -13.62342057059707 * Lastest 100 Episods Avg Reward is ==> -223.0371077809255\n",
            "Episode * 150 * Steps 999  * epsilon 0.8597827393003539 * Episodic Reward is ==> -53.23583001997444 * Lastest 100 Episods Avg Reward is ==> -196.97041858370835\n",
            "Episode * 160 * Steps 999  * epsilon 0.8512234991370281 * Episodic Reward is ==> -36.93690505912477 * Lastest 100 Episods Avg Reward is ==> -167.1988763209431\n",
            "Episode * 170 * Steps 999  * epsilon 0.8427494672348417 * Episodic Reward is ==> -34.784263925346856 * Lastest 100 Episods Avg Reward is ==> -138.9045674149644\n",
            "Episode * 180 * Steps 999  * epsilon 0.8343597953353479 * Episodic Reward is ==> -85.56516709414119 * Lastest 100 Episods Avg Reward is ==> -125.07528764422639\n",
            "Episode * 190 * Steps 999  * epsilon 0.8260536436246144 * Episodic Reward is ==> -80.3658303863974 * Lastest 100 Episods Avg Reward is ==> -90.8153809806575\n",
            "Episode * 200 * Steps 999  * epsilon 0.8178301806491574 * Episodic Reward is ==> -11.873401760131868 * Lastest 100 Episods Avg Reward is ==> -76.70784708787606\n",
            "Episode * 210 * Steps 191  * epsilon 0.8096885832327116 * Episodic Reward is ==> -57.90076752837839 * Lastest 100 Episods Avg Reward is ==> -49.297167588273716\n",
            "Episode * 220 * Steps 999  * epsilon 0.8016280363938307 * Episodic Reward is ==> 0.5192314386490937 * Lastest 100 Episods Avg Reward is ==> -35.42165475038347\n",
            "Episode * 230 * Steps 999  * epsilon 0.7936477332643059 * Episodic Reward is ==> -109.84342766257284 * Lastest 100 Episods Avg Reward is ==> -29.061169818663952\n",
            "Episode * 240 * Steps 999  * epsilon 0.7857468750083979 * Episodic Reward is ==> 3.5327665587370687 * Lastest 100 Episods Avg Reward is ==> -29.59228801261965\n",
            "Episode * 250 * Steps 74  * epsilon 0.7779246707428734 * Episodic Reward is ==> 44.89453239201083 * Lastest 100 Episods Avg Reward is ==> -12.122240261521826\n",
            "Episode * 260 * Steps 293  * epsilon 0.7701803374578359 * Episodic Reward is ==> 237.8146274679216 * Lastest 100 Episods Avg Reward is ==> 0.8986289463908187\n",
            "Episode * 270 * Steps 999  * epsilon 0.7625130999383466 * Episodic Reward is ==> -8.471446275463894 * Lastest 100 Episods Avg Reward is ==> 18.934783430104677\n",
            "Episode * 280 * Steps 463  * epsilon 0.7549221906868242 * Episodic Reward is ==> 212.81411324876083 * Lastest 100 Episods Avg Reward is ==> 36.988739309900126\n",
            "Episode * 290 * Steps 566  * epsilon 0.7474068498462175 * Episodic Reward is ==> 217.91272181269548 * Lastest 100 Episods Avg Reward is ==> 42.06102104913624\n",
            "Episode * 300 * Steps 999  * epsilon 0.7399663251239436 * Episodic Reward is ==> 22.40598266367666 * Lastest 100 Episods Avg Reward is ==> 49.55988948719282\n",
            "Episode * 310 * Steps 84  * epsilon 0.7325998717165821 * Episodic Reward is ==> 12.553645965599912 * Lastest 100 Episods Avg Reward is ==> 54.059197856222085\n",
            "Episode * 320 * Steps 436  * epsilon 0.7253067522353204 * Episodic Reward is ==> 200.37917865374885 * Lastest 100 Episods Avg Reward is ==> 73.98359959021752\n",
            "Episode * 330 * Steps 402  * epsilon 0.7180862366321393 * Episodic Reward is ==> 219.63192685773575 * Lastest 100 Episods Avg Reward is ==> 74.42158365814161\n",
            "Episode * 340 * Steps 204  * epsilon 0.7109376021267352 * Episodic Reward is ==> 270.8646681147133 * Lastest 100 Episods Avg Reward is ==> 92.8258535863893\n",
            "Episode * 350 * Steps 338  * epsilon 0.7038601331341691 * Episodic Reward is ==> 227.64713143697466 * Lastest 100 Episods Avg Reward is ==> 118.48932789161734\n",
            "Episode * 360 * Steps 612  * epsilon 0.6968531211932361 * Episodic Reward is ==> 242.5170515769127 * Lastest 100 Episods Avg Reward is ==> 126.61930331507574\n",
            "Episode * 370 * Steps 241  * epsilon 0.6899158648955466 * Episodic Reward is ==> 297.59322452246704 * Lastest 100 Episods Avg Reward is ==> 139.05017315805014\n",
            "Episode * 380 * Steps 411  * epsilon 0.6830476698153162 * Episodic Reward is ==> 247.67071525306162 * Lastest 100 Episods Avg Reward is ==> 138.33441742317345\n",
            "Episode * 390 * Steps 129  * epsilon 0.6762478484398523 * Episodic Reward is ==> 23.528324281596653 * Lastest 100 Episods Avg Reward is ==> 137.82590402031792\n",
            "Episode * 400 * Steps 999  * epsilon 0.6695157201007336 * Episodic Reward is ==> 134.25387120731563 * Lastest 100 Episods Avg Reward is ==> 157.7777404202556\n",
            "Episode * 410 * Steps 108  * epsilon 0.662850610905674 * Episodic Reward is ==> 8.676900329158315 * Lastest 100 Episods Avg Reward is ==> 163.93649182140769\n",
            "Episode * 420 * Steps 145  * epsilon 0.6562518536710664 * Episodic Reward is ==> 18.438943282840455 * Lastest 100 Episods Avg Reward is ==> 163.30284983218672\n",
            "Episode * 430 * Steps 149  * epsilon 0.6497187878551962 * Episodic Reward is ==> 19.406116236495038 * Lastest 100 Episods Avg Reward is ==> 174.01933395751644\n",
            "Episode * 440 * Steps 92  * epsilon 0.6432507594921204 * Episodic Reward is ==> 0.0062740883515886026 * Lastest 100 Episods Avg Reward is ==> 174.13293022603722\n",
            "Episode * 450 * Steps 211  * epsilon 0.6368471211262058 * Episodic Reward is ==> 231.89054198946522 * Lastest 100 Episods Avg Reward is ==> 174.1195710505724\n",
            "Episode * 460 * Steps 170  * epsilon 0.6305072317473174 * Episodic Reward is ==> 264.46874567222494 * Lastest 100 Episods Avg Reward is ==> 174.68516039661768\n",
            "Episode * 470 * Steps 999  * epsilon 0.6242304567266527 * Episodic Reward is ==> 161.7584177767425 * Lastest 100 Episods Avg Reward is ==> 174.6606884774513\n",
            "Episode * 480 * Steps 999  * epsilon 0.6180161677532153 * Episodic Reward is ==> 89.51496031532992 * Lastest 100 Episods Avg Reward is ==> 172.1303642912807\n",
            "Episode * 490 * Steps 384  * epsilon 0.6118637427709198 * Episodic Reward is ==> 266.3765838694498 * Lastest 100 Episods Avg Reward is ==> 164.84158133039284\n",
            "Episode * 500 * Steps 999  * epsilon 0.6057725659163237 * Episodic Reward is ==> 57.78660706768413 * Lastest 100 Episods Avg Reward is ==> 170.9430900700769\n",
            "Episode * 510 * Steps 999  * epsilon 0.5997420274569785 * Episodic Reward is ==> -234.1351028881402 * Lastest 100 Episods Avg Reward is ==> 173.5331028653549\n",
            "Episode * 520 * Steps 999  * epsilon 0.5937715237303958 * Episodic Reward is ==> 105.13596859990714 * Lastest 100 Episods Avg Reward is ==> 174.71096912205755\n",
            "Episode * 530 * Steps 123  * epsilon 0.5878604570836192 * Episodic Reward is ==> -16.141778322686008 * Lastest 100 Episods Avg Reward is ==> 181.69505126853997\n",
            "Episode * 540 * Steps 613  * epsilon 0.5820082358133997 * Episodic Reward is ==> 232.74965345320751 * Lastest 100 Episods Avg Reward is ==> 180.48745349282225\n",
            "Episode * 550 * Steps 289  * epsilon 0.576214274106964 * Episodic Reward is ==> 256.65940564579626 * Lastest 100 Episods Avg Reward is ==> 182.85264470006382\n",
            "Episode * 560 * Steps 267  * epsilon 0.5704779919833761 * Episodic Reward is ==> 0.25081886807181775 * Lastest 100 Episods Avg Reward is ==> 169.79270146390826\n",
            "Episode * 570 * Steps 502  * epsilon 0.5647988152354793 * Episodic Reward is ==> 246.34259770798556 * Lastest 100 Episods Avg Reward is ==> 172.20099721172434\n",
            "Episode * 580 * Steps 331  * epsilon 0.5591761753724176 * Episodic Reward is ==> 266.13145516589907 * Lastest 100 Episods Avg Reward is ==> 172.38729552134285\n",
            "Episode * 590 * Steps 999  * epsilon 0.5536095095627305 * Episodic Reward is ==> -136.20333012434963 * Lastest 100 Episods Avg Reward is ==> 149.35198482762056\n",
            "Episode * 600 * Steps 280  * epsilon 0.548098260578011 * Episodic Reward is ==> 287.8223558257312 * Lastest 100 Episods Avg Reward is ==> 136.7800057734098\n",
            "Episode * 610 * Steps 999  * epsilon 0.5426418767371284 * Episodic Reward is ==> 28.356436861149653 * Lastest 100 Episods Avg Reward is ==> 120.41159999852934\n",
            "Episode * 620 * Steps 178  * epsilon 0.5372398118510032 * Episodic Reward is ==> 76.26427303761437 * Lastest 100 Episods Avg Reward is ==> 98.12429195955833\n",
            "Episode * 630 * Steps 477  * epsilon 0.531891525167934 * Episodic Reward is ==> -171.13417842519345 * Lastest 100 Episods Avg Reward is ==> 80.76212842803433\n",
            "Episode * 640 * Steps 486  * epsilon 0.5265964813194676 * Episodic Reward is ==> 283.33702922711734 * Lastest 100 Episods Avg Reward is ==> 73.76094585972533\n",
            "Episode * 650 * Steps 281  * epsilon 0.5213541502668072 * Episodic Reward is ==> 245.7251975078448 * Lastest 100 Episods Avg Reward is ==> 52.38044533675165\n",
            "Episode * 660 * Steps 999  * epsilon 0.5161640072477562 * Episodic Reward is ==> -117.13170875600028 * Lastest 100 Episods Avg Reward is ==> 50.78903098477937\n",
            "Episode * 670 * Steps 999  * epsilon 0.5110255327241885 * Episodic Reward is ==> -83.31968262755836 * Lastest 100 Episods Avg Reward is ==> 32.106080982624775\n",
            "Episode * 680 * Steps 330  * epsilon 0.505938212330042 * Episodic Reward is ==> 192.63301136945 * Lastest 100 Episods Avg Reward is ==> 14.53132662282956\n",
            "Episode * 690 * Steps 184  * epsilon 0.5009015368198305 * Episodic Reward is ==> 216.69200823847316 * Lastest 100 Episods Avg Reward is ==> 24.653020637606016\n",
            "Episode * 700 * Steps 999  * epsilon 0.4959150020176678 * Episodic Reward is ==> -69.30975695194151 * Lastest 100 Episods Avg Reward is ==> 8.407586602514874\n",
            "Episode * 710 * Steps 521  * epsilon 0.4909781087667989 * Episodic Reward is ==> 194.11995728053734 * Lastest 100 Episods Avg Reward is ==> 14.547080775320996\n",
            "Episode * 720 * Steps 250  * epsilon 0.48609036287963414 * Episodic Reward is ==> 254.82243046488088 * Lastest 100 Episods Avg Reward is ==> 27.828192576626453\n",
            "Episode * 730 * Steps 293  * epsilon 0.48125127508828036 * Episodic Reward is ==> 267.261084149313 * Lastest 100 Episods Avg Reward is ==> 35.479602471424315\n",
            "Episode * 740 * Steps 699  * epsilon 0.47646036099556505 * Episodic Reward is ==> 226.8722748863185 * Lastest 100 Episods Avg Reward is ==> 44.379236512202326\n",
            "Episode * 750 * Steps 332  * epsilon 0.47171714102654755 * Episodic Reward is ==> 265.6183297332507 * Lastest 100 Episods Avg Reward is ==> 61.641677789365474\n",
            "Episode * 760 * Steps 999  * epsilon 0.4670211403805131 * Episodic Reward is ==> -103.37216062582118 * Lastest 100 Episods Avg Reward is ==> 76.66008942235533\n",
            "Episode * 770 * Steps 999  * epsilon 0.46237188898344517 * Episodic Reward is ==> -130.2871500269934 * Lastest 100 Episods Avg Reward is ==> 70.52400242313708\n",
            "Episode * 780 * Steps 999  * epsilon 0.45776892144096987 * Episodic Reward is ==> -203.0190751658852 * Lastest 100 Episods Avg Reward is ==> 58.0240110296993\n",
            "Episode * 790 * Steps 534  * epsilon 0.45321177699177073 * Episodic Reward is ==> 226.50530616230395 * Lastest 100 Episods Avg Reward is ==> 57.6067394335548\n",
            "Episode * 800 * Steps 386  * epsilon 0.44869999946146477 * Episodic Reward is ==> -162.19603338504666 * Lastest 100 Episods Avg Reward is ==> 53.45648049652921\n",
            "Episode * 810 * Steps 999  * epsilon 0.44423313721693997 * Episodic Reward is ==> -153.48088080829413 * Lastest 100 Episods Avg Reward is ==> 43.1322942123601\n",
            "Episode * 820 * Steps 168  * epsilon 0.43981074312114604 * Episodic Reward is ==> 56.8827293448534 * Lastest 100 Episods Avg Reward is ==> 26.621979662081714\n",
            "Episode * 830 * Steps 176  * epsilon 0.4354323744883354 * Episodic Reward is ==> 19.297366949260166 * Lastest 100 Episods Avg Reward is ==> 7.419739719445704\n",
            "Episode * 840 * Steps 999  * epsilon 0.4310975930397502 * Episodic Reward is ==> 5.3517100423925825 * Lastest 100 Episods Avg Reward is ==> -15.065726007682253\n",
            "Episode * 850 * Steps 255  * epsilon 0.4268059648597502 * Episodic Reward is ==> -51.23392392267951 * Lastest 100 Episods Avg Reward is ==> -44.78367825923938\n",
            "Episode * 860 * Steps 106  * epsilon 0.42255706035237733 * Episodic Reward is ==> 31.03626234341334 * Lastest 100 Episods Avg Reward is ==> -62.37643753126494\n",
            "Episode * 870 * Steps 777  * epsilon 0.41835045419835276 * Episodic Reward is ==> -148.5832786165907 * Lastest 100 Episods Avg Reward is ==> -64.66243579860382\n",
            "Episode * 880 * Steps 226  * epsilon 0.4141857253125019 * Episodic Reward is ==> 264.65614398576747 * Lastest 100 Episods Avg Reward is ==> -43.011363860818825\n",
            "Episode * 890 * Steps 170  * epsilon 0.41006245680160364 * Episodic Reward is ==> 259.69836521082715 * Lastest 100 Episods Avg Reward is ==> -34.291488537609965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuF4rIYqwyZ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}