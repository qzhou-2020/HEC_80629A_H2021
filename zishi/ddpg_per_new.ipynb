{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg_per_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otdpa1QZV19H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7907e897-aea3-450d-f364-cdb0abddca12"
      },
      "source": [
        "!pip install --upgrade --force-reinstall box2d-py\n",
        "!pip install --upgrade --force-reinstall gym[Box_2D]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting gym[Box_2D]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 5.8MB/s \n",
            "\u001b[33m  WARNING: gym 0.18.0 does not provide the extra 'box_2d'\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/91/ee427c42957f8c4cbe477bf4f8b7f608e003a17941e509d1777e58648cb3/scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 108kB/s \n",
            "\u001b[?25hCollecting numpy>=1.10.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 127kB/s \n",
            "\u001b[?25hCollecting pyglet<=1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.5MB/s \n",
            "\u001b[?25hCollecting Pillow<=7.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 41.0MB/s \n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Collecting future\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 22.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.18.0-cp37-none-any.whl size=1656450 sha256=23464fcc1434e7f3c0aacb58646f15efa06c5277bac1c881f17b61d5db04bcb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=cf970b6858d5e06c725dfb9ce3afe3ba525cff21032b894be66097d0862569cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built gym future\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scipy, future, pyglet, Pillow, cloudpickle, gym\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 future-0.18.2 gym-0.18.0 numpy-1.20.2 pyglet-1.5.0 scipy-1.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpaMSP_LwgQp"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNXpnq0jt4uO"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,activations\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "RANDOM_SEEDS = 123\n",
        "\n",
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZi8mrsrt9HJ"
      },
      "source": [
        "# Ornstein-Uhlenbeck process\n",
        "class OUNoise:\n",
        "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = (self.x_prev \n",
        "            + self.theta * (self.mu - self.x_prev) * self.dt \n",
        "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape))\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x_initial if self.x_initial is not None else np.zeros_like(self.mu)\n",
        "\n",
        "class SumTree(object):\n",
        "    \"\"\"\n",
        "    This SumTree code is a modified version and the original code is from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
        "    Story data with its priority in the tree.\n",
        "    \"\"\"\n",
        "    data_pointer = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity  # for all priority values\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
        "        #             size: capacity - 1                       size: capacity\n",
        "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
        "        # [--------------data frame-------------]\n",
        "        #             size: capacity\n",
        "\n",
        "    def add(self, p, data):\n",
        "        tree_idx = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data  # update data_frame\n",
        "        self.update(tree_idx, p)  # update tree_frame\n",
        "\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_idx, p):\n",
        "        change = p - self.tree[tree_idx]\n",
        "        self.tree[tree_idx] = p\n",
        "        # then propagate the change through tree\n",
        "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
        "            tree_idx = (tree_idx - 1) // 2\n",
        "            self.tree[tree_idx] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"\n",
        "        Tree structure and array storage:\n",
        "        Tree index:\n",
        "             0         -> storing priority sum\n",
        "            / \\\n",
        "          1     2\n",
        "         / \\   / \\\n",
        "        3   4 5   6    -> storing priority for transitions\n",
        "        Array type for storing:\n",
        "        [0,1,2,3,4,5,6]\n",
        "        \"\"\"\n",
        "        parent_idx = 0\n",
        "        while True:     # the while loop is faster than the method in the reference code\n",
        "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
        "            cr_idx = cl_idx + 1\n",
        "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
        "                leaf_idx = parent_idx\n",
        "                break\n",
        "            else:       # downward search, always search for a higher priority node\n",
        "                if v <= self.tree[cl_idx]:\n",
        "                    parent_idx = cl_idx\n",
        "                else:\n",
        "                    v -= self.tree[cl_idx]\n",
        "                    parent_idx = cr_idx\n",
        "\n",
        "        data_idx = leaf_idx - self.capacity + 1\n",
        "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
        "\n",
        "    @property\n",
        "    def total_p(self):\n",
        "        return self.tree[0]  # the root\n",
        "\n",
        "\n",
        "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    \"\"\"\n",
        "    This Memory class is modified based on the original code from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
        "    \"\"\"\n",
        "    epsilon = 0.01  # small amount to avoid zero priority\n",
        "    alpha = 0.7  # [0~1] convert the importance of TD error to priority\n",
        "    beta = 0.5  # importance-sampling, from initial value increasing to 1\n",
        "    beta_increment_per_sampling = 0.001\n",
        "    abs_err_upper = 1.  # clipped abs error\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.sample_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sample_count\n",
        "\n",
        "    def store(self, transition):\n",
        "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
        "        if max_p == 0:\n",
        "            max_p = self.abs_err_upper\n",
        "        self.tree.add(max_p, transition)   # set the max p for new p\n",
        "        self.sample_count = min(self.sample_count + 1, self.tree.capacity)\n",
        "\n",
        "    def sample(self, n):\n",
        "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
        "        pri_seg = self.tree.total_p / n       # priority segment\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
        "\n",
        "        a = self.tree.tree[-self.tree.capacity:]\n",
        "        min_prob = np.min(a[a != 0]) / self.tree.total_p     # for later calculate ISweight\n",
        "        for i in range(n):\n",
        "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
        "            v = np.random.uniform(a, b)\n",
        "            idx, p, data = self.tree.get_leaf(v)\n",
        "            prob = p / self.tree.total_p\n",
        "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
        "            b_idx[i], b_memory[i, :] = idx, data\n",
        "\n",
        "        return b_idx, b_memory, ISWeights\n",
        "\n",
        "    def batch_update(self, tree_idx, abs_errors):\n",
        "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
        "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
        "        ps = np.power(clipped_errors, self.alpha)\n",
        "        for ti, p in zip(tree_idx, ps):\n",
        "            self.tree.update(ti, p)\n",
        "\n",
        "\n",
        "def get_actor(state_shape, action_dim, upper_bound, units=(512,512)):\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.004, maxval=0.004)\n",
        "    inputs = layers.Input(shape=(state_shape,))\n",
        "    for idx, unit in enumerate(units):\n",
        "        if idx == 0:\n",
        "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(inputs)\n",
        "        else:\n",
        "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(action_dim, name=\"Output\", activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "    scaled_outputs = outputs * upper_bound\n",
        "    model = Model(inputs,scaled_outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic(state_shape, action_dim, state_units=(512,256),action_units=128,concat_units=128):\n",
        "    state_input = layers.Input(shape=(state_shape), name=\"state_input\")\n",
        "    for idx, unit in enumerate(state_units):      \n",
        "        if idx == 0:\n",
        "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_input)\n",
        "        else:\n",
        "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_out)\n",
        "    \n",
        "    action_input = layers.Input(shape=(action_dim),name=\"action_input\")\n",
        "    action_out = layers.Dense(action_units, name=\"action_lvl\", activation=\"relu\")(action_input)\n",
        "    \n",
        "    concat = layers.Concatenate()([state_out,action_out])\n",
        "    out = layers.Dense(concat_units, name=\"concat_lvl\",activation=\"relu\")(concat)\n",
        "    outputs = layers.Dense(1)(out) # Q value\n",
        "\n",
        "    model = Model([state_input, action_input], outputs)\n",
        "    return model\n",
        "\n",
        "@tf.function\n",
        "def update_target(model, target_model, tau=0.001):\n",
        "    weights = model.variables\n",
        "    target_weights = target_model.variables\n",
        "    for (a,b) in zip(target_weights,weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP_5PWKuhj6T"
      },
      "source": [
        "class DDPG:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 per=False,\n",
        "                 actor_lr=0.00005,\n",
        "                 critic_lr=0.0005,\n",
        "                 actor_units=(512,512),\n",
        "                 state_units=(512,512),\n",
        "                 action_units=128,\n",
        "                 concat_units=256,\n",
        "                 noise=\"OU\",\n",
        "                 tau=0.001,\n",
        "                 gamma=0.99,\n",
        "                 batch_size=256,\n",
        "                 memory_size=2**20\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.per = per\n",
        "        self.state_shape = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.upper_bound = env.action_space.high[0]\n",
        "        self.lower_bound = env.action_space.low[0]\n",
        "        if noise == \"OU\":\n",
        "            self.noise = OUNoise(mu=np.zeros(self.action_dim),sigma=float(0.2)*np.ones(self.action_dim))\n",
        "        else:\n",
        "            self.noise = np.random.normal(scale=0.2, size=self.action_dim)\n",
        "        self.actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
        "        self.critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
        "        self.target_actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
        "        self.target_critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
        "        self.target_actor_model.set_weights(self.actor_model.get_weights())\n",
        "        self.target_critic_model.set_weights(self.critic_model.get_weights())\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = Memory(capacity=memory_size) if per else deque(maxlen=memory_size)\n",
        "\n",
        "    def policy(self, state, noise_object):\n",
        "        sampled_actions = tf.squeeze(self.actor_model(state))\n",
        "\n",
        "        noise = noise_object()\n",
        "        sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "        legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
        "        return np.squeeze(legal_action)\n",
        "\n",
        "    def record(self, obs_array):\n",
        "        if self.per:\n",
        "            transition = np.hstack(obs_array)\n",
        "            self.memory.store(transition)\n",
        "        else:\n",
        "            self.memory.append(obs_array)    \n",
        "\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def per_update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights\n",
        "    ):\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
        "            y = reward_batch + self.gamma * self.target_critic_model(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            ) * (1. - done_batch)\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            errors = y - critic_value\n",
        "            critic_loss = tf.math.reduce_mean(ISWeights * tf.math.square(errors))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, self.critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        abs_errors = tf.reduce_sum(tf.abs(errors), axis=1)\n",
        "        #self.memory.batch_update(tree_idx, abs_errors)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor_model.trainable_variables)\n",
        "        )\n",
        "        return abs_errors\n",
        "\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "    ):\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
        "            y = reward_batch + self.gamma * self.target_critic_model(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            ) * (1. - done_batch)\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.keras.losses.Huber()(y,critic_value)\n",
        "\n",
        "            critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "            self.critic_optimizer.apply_gradients(\n",
        "                zip(critic_grad, self.critic_model.trainable_variables)\n",
        "            )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        if self.per:\n",
        "            tree_idx, samples, ISWeights = self.memory.sample(min(self.batch_size,len(self.memory)))\n",
        "            ISWeights_batch = tf.convert_to_tensor(ISWeights,dtype=tf.float32)\n",
        "            split_shape = np.cumsum([self.state_shape, self.action_dim, 1, self.state_shape])\n",
        "            states, actions, rewards, next_states, dones = np.hsplit(samples, split_shape)\n",
        "            state_batch = tf.convert_to_tensor(states)\n",
        "            action_batch = tf.convert_to_tensor(actions)\n",
        "            reward_batch = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(next_states)\n",
        "            done_batch = tf.convert_to_tensor(dones,dtype=tf.float32)\n",
        "            abs_errors = self.per_update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights_batch)\n",
        "            self.memory.batch_update(tree_idx, abs_errors.numpy())\n",
        "        else:\n",
        "            samples = random.sample(self.memory, min(self.batch_size,len(self.memory)))\n",
        "            trans_s = np.array(samples,dtype=object).T\n",
        "            state_batch = tf.convert_to_tensor(np.row_stack(trans_s[0]))\n",
        "            action_batch = tf.convert_to_tensor(np.row_stack(trans_s[1]))\n",
        "            reward_batch = tf.convert_to_tensor(np.row_stack(trans_s[2]),dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(np.row_stack(trans_s[3]))\n",
        "            done_batch = tf.convert_to_tensor(np.row_stack(trans_s[4]),dtype=tf.float32)\n",
        "            self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "    def train(self, max_episodes=2000, max_steps=1000):\n",
        "        self.ep_reward_list = []\n",
        "        self.avg_reward_list = []\n",
        "        self.ep_steps_list = []\n",
        "\n",
        "        for ep in range(max_episodes):\n",
        "            prev_state = self.env.reset()\n",
        "            episodic_reward = 0\n",
        "            steps = 0\n",
        "            \n",
        "            while steps < max_steps:\n",
        "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "                action = self.policy(tf_prev_state, self.noise)\n",
        "\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                self.record([prev_state, action, reward, state, done])\n",
        "\n",
        "                episodic_reward += reward\n",
        "                self.replay()\n",
        "\n",
        "                update_target(self.actor_model, self.target_actor_model, self.tau)\n",
        "                update_target(self.critic_model, self.target_critic_model, self.tau)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                prev_state = state\n",
        "                steps += 1\n",
        "            self.ep_reward_list.append(episodic_reward)\n",
        "            # Mean of last 100 episodes\n",
        "            avg_reward = np.mean(self.ep_reward_list[-100:])\n",
        "            self.avg_reward_list.append(avg_reward)\n",
        "            self.ep_steps_list.append(steps)\n",
        "            avg_step = np.mean(self.ep_steps_list[-100:])\n",
        "            if ep%10 == 0:\n",
        "                print(f\"Episode * {ep} * Avg Steps {avg_step} * Episodic Reward is ==> {episodic_reward} * Lastest 100 Episods Avg Reward is ==> {avg_reward}\")\n",
        "\n",
        "\n",
        "        plt.plot(self.avg_reward_list)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "        plt.show()\n",
        "\n",
        "        content = {\"ep_reward\":self.ep_reward_list,\"avg_reward\":self.avg_reward_list, \"steps\":self.ep_steps_list}\n",
        "        df = pd.DataFrame(content)\n",
        "        df.to_csv(f'ddpg_per_{self.per}_{date_time}.csv') \n",
        "        files.download(f'ddpg_per_{self.per}_{date_time}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmmC-HiYh16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418a3eec-d5af-4b6f-d3fb-983257df0c2e"
      },
      "source": [
        "problem = 'LunarLanderContinuous-v2'\n",
        "gym_env = gym.make(problem)\n",
        "gym_env.seed(RANDOM_SEEDS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkJwfK0hvvw"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "ddpg = DDPG(gym_env,per=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fb2tBUrvjynI",
        "outputId": "a2fb4f55-a709-45f8-9cce-3905eea46900"
      },
      "source": [
        "ddpg.train(max_episodes=2000,max_steps=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Avg Steps 85.0 * Episodic Reward is ==> -38.98723359888106 * Lastest 100 Episods Avg Reward is ==> -38.98723359888106\n",
            "Episode * 10 * Avg Steps 94.27272727272727 * Episodic Reward is ==> -573.8740710038464 * Lastest 100 Episods Avg Reward is ==> -752.2294821238579\n",
            "Episode * 20 * Avg Steps 88.23809523809524 * Episodic Reward is ==> -610.8685863462115 * Lastest 100 Episods Avg Reward is ==> -781.7061040929841\n",
            "Episode * 30 * Avg Steps 82.93548387096774 * Episodic Reward is ==> -851.6327485861467 * Lastest 100 Episods Avg Reward is ==> -745.8424666489403\n",
            "Episode * 40 * Avg Steps 80.70731707317073 * Episodic Reward is ==> -1461.6018267607983 * Lastest 100 Episods Avg Reward is ==> -727.5453822324325\n",
            "Episode * 50 * Avg Steps 76.41176470588235 * Episodic Reward is ==> -396.4509055994927 * Lastest 100 Episods Avg Reward is ==> -685.7241322629393\n",
            "Episode * 60 * Avg Steps 78.06557377049181 * Episodic Reward is ==> -516.6639098961111 * Lastest 100 Episods Avg Reward is ==> -675.1666200878431\n",
            "Episode * 70 * Avg Steps 76.77464788732394 * Episodic Reward is ==> -821.5499735816901 * Lastest 100 Episods Avg Reward is ==> -660.8824686793007\n",
            "Episode * 80 * Avg Steps 79.79012345679013 * Episodic Reward is ==> -375.4741049798774 * Lastest 100 Episods Avg Reward is ==> -611.2138022254536\n",
            "Episode * 90 * Avg Steps 85.12087912087912 * Episodic Reward is ==> -431.7801322066213 * Lastest 100 Episods Avg Reward is ==> -576.1639798343153\n",
            "Episode * 100 * Avg Steps 86.05 * Episodic Reward is ==> -350.84654164751396 * Lastest 100 Episods Avg Reward is ==> -544.445050839319\n",
            "Episode * 110 * Avg Steps 85.67 * Episodic Reward is ==> -160.3994240412161 * Lastest 100 Episods Avg Reward is ==> -489.25204108789194\n",
            "Episode * 120 * Avg Steps 86.5 * Episodic Reward is ==> -128.59156275271403 * Lastest 100 Episods Avg Reward is ==> -422.0676423129922\n",
            "Episode * 130 * Avg Steps 87.44 * Episodic Reward is ==> -20.763953194258164 * Lastest 100 Episods Avg Reward is ==> -371.2577651349536\n",
            "Episode * 140 * Avg Steps 88.15 * Episodic Reward is ==> -155.3214573122769 * Lastest 100 Episods Avg Reward is ==> -321.373906448118\n",
            "Episode * 150 * Avg Steps 89.3 * Episodic Reward is ==> -106.32963763552229 * Lastest 100 Episods Avg Reward is ==> -283.47044580703374\n",
            "Episode * 160 * Avg Steps 88.68 * Episodic Reward is ==> -291.5970979558798 * Lastest 100 Episods Avg Reward is ==> -236.27639157099276\n",
            "Episode * 170 * Avg Steps 89.45 * Episodic Reward is ==> -29.773189407580006 * Lastest 100 Episods Avg Reward is ==> -189.06096678707817\n",
            "Episode * 180 * Avg Steps 87.68 * Episodic Reward is ==> -52.37401201163655 * Lastest 100 Episods Avg Reward is ==> -178.5339277500148\n",
            "Episode * 190 * Avg Steps 84.12 * Episodic Reward is ==> -345.1094459102334 * Lastest 100 Episods Avg Reward is ==> -168.52157346911554\n",
            "Episode * 200 * Avg Steps 81.86 * Episodic Reward is ==> -330.3459036112914 * Lastest 100 Episods Avg Reward is ==> -180.87426713506818\n",
            "Episode * 210 * Avg Steps 82.52 * Episodic Reward is ==> -128.35496674663224 * Lastest 100 Episods Avg Reward is ==> -177.10630819372957\n",
            "Episode * 220 * Avg Steps 84.81 * Episodic Reward is ==> -279.6719419927212 * Lastest 100 Episods Avg Reward is ==> -191.48916092341318\n",
            "Episode * 230 * Avg Steps 87.06 * Episodic Reward is ==> -166.99254363871245 * Lastest 100 Episods Avg Reward is ==> -191.01329227516115\n",
            "Episode * 240 * Avg Steps 92.74 * Episodic Reward is ==> -390.22235368602924 * Lastest 100 Episods Avg Reward is ==> -198.5157380843154\n",
            "Episode * 250 * Avg Steps 98.51 * Episodic Reward is ==> -373.2429237876028 * Lastest 100 Episods Avg Reward is ==> -209.17201577310087\n",
            "Episode * 260 * Avg Steps 98.95 * Episodic Reward is ==> -66.1825519993964 * Lastest 100 Episods Avg Reward is ==> -202.93997812222196\n",
            "Episode * 270 * Avg Steps 105.89 * Episodic Reward is ==> -523.0839985629759 * Lastest 100 Episods Avg Reward is ==> -214.04048882878004\n",
            "Episode * 280 * Avg Steps 113.0 * Episodic Reward is ==> -262.3850209673145 * Lastest 100 Episods Avg Reward is ==> -210.37787590868186\n",
            "Episode * 290 * Avg Steps 111.82 * Episodic Reward is ==> -92.99476055988613 * Lastest 100 Episods Avg Reward is ==> -198.08413902114597\n",
            "Episode * 300 * Avg Steps 120.26 * Episodic Reward is ==> -22.3143198839508 * Lastest 100 Episods Avg Reward is ==> -184.57213685256465\n",
            "Episode * 310 * Avg Steps 126.53 * Episodic Reward is ==> -124.51745205543189 * Lastest 100 Episods Avg Reward is ==> -177.816811389008\n",
            "Episode * 320 * Avg Steps 133.66 * Episodic Reward is ==> 32.545821840914016 * Lastest 100 Episods Avg Reward is ==> -151.11554732842768\n",
            "Episode * 330 * Avg Steps 144.03 * Episodic Reward is ==> -9.750337729469564 * Lastest 100 Episods Avg Reward is ==> -141.99194421698374\n",
            "Episode * 340 * Avg Steps 146.41 * Episodic Reward is ==> -1.6416644475951188 * Lastest 100 Episods Avg Reward is ==> -116.47204153066576\n",
            "Episode * 350 * Avg Steps 152.35 * Episodic Reward is ==> 10.424647508204629 * Lastest 100 Episods Avg Reward is ==> -97.4334089129463\n",
            "Episode * 360 * Avg Steps 161.55 * Episodic Reward is ==> 0.24170232622843457 * Lastest 100 Episods Avg Reward is ==> -99.42082509102309\n",
            "Episode * 370 * Avg Steps 165.74 * Episodic Reward is ==> -167.95676444319804 * Lastest 100 Episods Avg Reward is ==> -91.54291930385773\n",
            "Episode * 380 * Avg Steps 164.86 * Episodic Reward is ==> -167.3732683456965 * Lastest 100 Episods Avg Reward is ==> -98.66484686208182\n",
            "Episode * 390 * Avg Steps 177.01 * Episodic Reward is ==> -122.63064819896437 * Lastest 100 Episods Avg Reward is ==> -108.27366969627884\n",
            "Episode * 400 * Avg Steps 177.66 * Episodic Reward is ==> 81.13192352196342 * Lastest 100 Episods Avg Reward is ==> -101.27050484155417\n",
            "Episode * 410 * Avg Steps 175.59 * Episodic Reward is ==> -139.55991977176666 * Lastest 100 Episods Avg Reward is ==> -100.24775270171453\n",
            "Episode * 420 * Avg Steps 177.95 * Episodic Reward is ==> 20.70889746477494 * Lastest 100 Episods Avg Reward is ==> -108.58608928328167\n",
            "Episode * 430 * Avg Steps 184.49 * Episodic Reward is ==> -17.601129238596524 * Lastest 100 Episods Avg Reward is ==> -102.00119510048484\n",
            "Episode * 440 * Avg Steps 194.11 * Episodic Reward is ==> -4.285477983723424 * Lastest 100 Episods Avg Reward is ==> -114.53555612898853\n",
            "Episode * 450 * Avg Steps 195.31 * Episodic Reward is ==> 77.99931213146913 * Lastest 100 Episods Avg Reward is ==> -128.38701722569618\n",
            "Episode * 460 * Avg Steps 206.39 * Episodic Reward is ==> 33.95460494665297 * Lastest 100 Episods Avg Reward is ==> -118.22024207714041\n",
            "Episode * 470 * Avg Steps 206.94 * Episodic Reward is ==> -461.90692076097645 * Lastest 100 Episods Avg Reward is ==> -116.09554708576468\n",
            "Episode * 480 * Avg Steps 211.93 * Episodic Reward is ==> -261.78204368882064 * Lastest 100 Episods Avg Reward is ==> -112.40918769255204\n",
            "Episode * 490 * Avg Steps 218.7 * Episodic Reward is ==> 96.10808414083401 * Lastest 100 Episods Avg Reward is ==> -99.3348844209292\n",
            "Episode * 500 * Avg Steps 222.97 * Episodic Reward is ==> 56.23478077353265 * Lastest 100 Episods Avg Reward is ==> -93.18018290116898\n",
            "Episode * 510 * Avg Steps 232.54 * Episodic Reward is ==> -151.67432023328445 * Lastest 100 Episods Avg Reward is ==> -82.81151737676822\n",
            "Episode * 520 * Avg Steps 239.93 * Episodic Reward is ==> 53.90042793122323 * Lastest 100 Episods Avg Reward is ==> -71.63739136916227\n",
            "Episode * 530 * Avg Steps 230.72 * Episodic Reward is ==> -84.08239768368189 * Lastest 100 Episods Avg Reward is ==> -90.82189090551348\n",
            "Episode * 540 * Avg Steps 226.93 * Episodic Reward is ==> 137.298245238839 * Lastest 100 Episods Avg Reward is ==> -89.90798189226668\n",
            "Episode * 550 * Avg Steps 225.65 * Episodic Reward is ==> -48.68881288137275 * Lastest 100 Episods Avg Reward is ==> -81.33708052871006\n",
            "Episode * 560 * Avg Steps 213.18 * Episodic Reward is ==> -161.3342426033808 * Lastest 100 Episods Avg Reward is ==> -94.40518736595996\n",
            "Episode * 570 * Avg Steps 216.37 * Episodic Reward is ==> -96.90189081703602 * Lastest 100 Episods Avg Reward is ==> -98.3296836128966\n",
            "Episode * 580 * Avg Steps 223.95 * Episodic Reward is ==> -118.5710868449279 * Lastest 100 Episods Avg Reward is ==> -97.09802403465429\n",
            "Episode * 590 * Avg Steps 223.59 * Episodic Reward is ==> -122.41131117349471 * Lastest 100 Episods Avg Reward is ==> -105.89206317642383\n",
            "Episode * 600 * Avg Steps 223.69 * Episodic Reward is ==> -149.69349947434017 * Lastest 100 Episods Avg Reward is ==> -110.66515807956124\n",
            "Episode * 610 * Avg Steps 224.27 * Episodic Reward is ==> -121.71651334066658 * Lastest 100 Episods Avg Reward is ==> -117.59995519214607\n",
            "Episode * 620 * Avg Steps 224.02 * Episodic Reward is ==> -0.37109355965060475 * Lastest 100 Episods Avg Reward is ==> -122.90667421086364\n",
            "Episode * 630 * Avg Steps 234.21 * Episodic Reward is ==> -4.349895307557404 * Lastest 100 Episods Avg Reward is ==> -108.06976921535268\n",
            "Episode * 640 * Avg Steps 241.01 * Episodic Reward is ==> 28.706246796957174 * Lastest 100 Episods Avg Reward is ==> -94.56608519250756\n",
            "Episode * 650 * Avg Steps 245.12 * Episodic Reward is ==> 8.741917972731628 * Lastest 100 Episods Avg Reward is ==> -82.73767603817855\n",
            "Episode * 660 * Avg Steps 252.67 * Episodic Reward is ==> 65.26439580600304 * Lastest 100 Episods Avg Reward is ==> -64.75223820420624\n",
            "Episode * 670 * Avg Steps 259.45 * Episodic Reward is ==> 60.50756508930446 * Lastest 100 Episods Avg Reward is ==> -44.94238890186607\n",
            "Episode * 680 * Avg Steps 259.43 * Episodic Reward is ==> 106.81627044188107 * Lastest 100 Episods Avg Reward is ==> -29.828129134141186\n",
            "Episode * 690 * Avg Steps 260.96 * Episodic Reward is ==> 81.29965146772386 * Lastest 100 Episods Avg Reward is ==> -18.523659607864023\n",
            "Episode * 700 * Avg Steps 270.32 * Episodic Reward is ==> 48.70976439906732 * Lastest 100 Episods Avg Reward is ==> -3.7162239117116034\n",
            "Episode * 710 * Avg Steps 275.41 * Episodic Reward is ==> -139.1288369286095 * Lastest 100 Episods Avg Reward is ==> 10.358845189734238\n",
            "Episode * 720 * Avg Steps 275.41 * Episodic Reward is ==> -135.7948299651448 * Lastest 100 Episods Avg Reward is ==> 18.397569392034402\n",
            "Episode * 730 * Avg Steps 270.87 * Episodic Reward is ==> 107.07528619441587 * Lastest 100 Episods Avg Reward is ==> 25.044210619840868\n",
            "Episode * 740 * Avg Steps 264.95 * Episodic Reward is ==> -64.4869735331059 * Lastest 100 Episods Avg Reward is ==> 14.696324509542514\n",
            "Episode * 750 * Avg Steps 264.51 * Episodic Reward is ==> 68.91851946589614 * Lastest 100 Episods Avg Reward is ==> 7.148821463919918\n",
            "Episode * 760 * Avg Steps 265.69 * Episodic Reward is ==> 52.750465893611114 * Lastest 100 Episods Avg Reward is ==> -1.2475183742402456\n",
            "Episode * 770 * Avg Steps 258.86 * Episodic Reward is ==> 97.5453107830265 * Lastest 100 Episods Avg Reward is ==> -7.325631006230549\n",
            "Episode * 780 * Avg Steps 257.29 * Episodic Reward is ==> -174.81646549169358 * Lastest 100 Episods Avg Reward is ==> -8.156808708136587\n",
            "Episode * 790 * Avg Steps 248.29 * Episodic Reward is ==> 83.06922504920782 * Lastest 100 Episods Avg Reward is ==> -16.837110615851856\n",
            "Episode * 800 * Avg Steps 239.69 * Episodic Reward is ==> 0.19141923336685363 * Lastest 100 Episods Avg Reward is ==> -36.22601946247213\n",
            "Episode * 810 * Avg Steps 235.63 * Episodic Reward is ==> 90.67835343875666 * Lastest 100 Episods Avg Reward is ==> -35.69569151377208\n",
            "Episode * 820 * Avg Steps 233.14 * Episodic Reward is ==> -230.45472800983387 * Lastest 100 Episods Avg Reward is ==> -45.08769962413426\n",
            "Episode * 830 * Avg Steps 230.81 * Episodic Reward is ==> 8.306756225617221 * Lastest 100 Episods Avg Reward is ==> -46.06148546488\n",
            "Episode * 840 * Avg Steps 231.55 * Episodic Reward is ==> 61.70348615570922 * Lastest 100 Episods Avg Reward is ==> -39.748705469841454\n",
            "Episode * 850 * Avg Steps 235.85 * Episodic Reward is ==> 63.33560050538158 * Lastest 100 Episods Avg Reward is ==> -26.41959298856999\n",
            "Episode * 860 * Avg Steps 239.0 * Episodic Reward is ==> 93.02499365096939 * Lastest 100 Episods Avg Reward is ==> -23.88264047264496\n",
            "Episode * 870 * Avg Steps 246.52 * Episodic Reward is ==> -8.607632033538467 * Lastest 100 Episods Avg Reward is ==> -22.21576888274145\n",
            "Episode * 880 * Avg Steps 241.67 * Episodic Reward is ==> -193.821962476499 * Lastest 100 Episods Avg Reward is ==> -38.8991233985041\n",
            "Episode * 890 * Avg Steps 240.47 * Episodic Reward is ==> 29.53318815930205 * Lastest 100 Episods Avg Reward is ==> -39.41449129120274\n",
            "Episode * 900 * Avg Steps 237.78 * Episodic Reward is ==> -181.4980964996197 * Lastest 100 Episods Avg Reward is ==> -27.85182215654831\n",
            "Episode * 910 * Avg Steps 239.74 * Episodic Reward is ==> -291.45746857586533 * Lastest 100 Episods Avg Reward is ==> -41.63066468705799\n",
            "Episode * 920 * Avg Steps 244.32 * Episodic Reward is ==> 16.464414843774207 * Lastest 100 Episods Avg Reward is ==> -33.04939479447824\n",
            "Episode * 930 * Avg Steps 242.44 * Episodic Reward is ==> -408.54068206092165 * Lastest 100 Episods Avg Reward is ==> -52.17262705501743\n",
            "Episode * 940 * Avg Steps 236.79 * Episodic Reward is ==> -48.93480451652627 * Lastest 100 Episods Avg Reward is ==> -60.61500339809112\n",
            "Episode * 950 * Avg Steps 235.9 * Episodic Reward is ==> -454.4868477319728 * Lastest 100 Episods Avg Reward is ==> -73.63459032012685\n",
            "Episode * 960 * Avg Steps 233.92 * Episodic Reward is ==> 42.031304112471965 * Lastest 100 Episods Avg Reward is ==> -76.04617828565303\n",
            "Episode * 970 * Avg Steps 227.02 * Episodic Reward is ==> -251.75839384361535 * Lastest 100 Episods Avg Reward is ==> -87.36816799432512\n",
            "Episode * 980 * Avg Steps 234.96 * Episodic Reward is ==> 76.8278807301452 * Lastest 100 Episods Avg Reward is ==> -71.19626270568935\n",
            "Episode * 990 * Avg Steps 241.69 * Episodic Reward is ==> 48.63549946013655 * Lastest 100 Episods Avg Reward is ==> -66.45964538028628\n",
            "Episode * 1000 * Avg Steps 249.96 * Episodic Reward is ==> 68.16953970424815 * Lastest 100 Episods Avg Reward is ==> -67.58317534932748\n",
            "Episode * 1010 * Avg Steps 242.44 * Episodic Reward is ==> -236.3381180648796 * Lastest 100 Episods Avg Reward is ==> -67.98998521050738\n",
            "Episode * 1020 * Avg Steps 231.59 * Episodic Reward is ==> -24.625959130339535 * Lastest 100 Episods Avg Reward is ==> -80.37252981763733\n",
            "Episode * 1030 * Avg Steps 223.8 * Episodic Reward is ==> -506.74427373177775 * Lastest 100 Episods Avg Reward is ==> -78.97296825790787\n",
            "Episode * 1040 * Avg Steps 215.28 * Episodic Reward is ==> -86.91253742693166 * Lastest 100 Episods Avg Reward is ==> -93.6654004338688\n",
            "Episode * 1050 * Avg Steps 201.02 * Episodic Reward is ==> -30.459458577335425 * Lastest 100 Episods Avg Reward is ==> -105.6524808469084\n",
            "Episode * 1060 * Avg Steps 193.06 * Episodic Reward is ==> 64.7980433514675 * Lastest 100 Episods Avg Reward is ==> -110.16907260406086\n",
            "Episode * 1070 * Avg Steps 190.27 * Episodic Reward is ==> -191.69448027134385 * Lastest 100 Episods Avg Reward is ==> -114.53440005805612\n",
            "Episode * 1080 * Avg Steps 182.88 * Episodic Reward is ==> -312.21193057304265 * Lastest 100 Episods Avg Reward is ==> -127.57049441139428\n",
            "Episode * 1090 * Avg Steps 180.56 * Episodic Reward is ==> -565.959208520004 * Lastest 100 Episods Avg Reward is ==> -151.28616626320743\n",
            "Episode * 1100 * Avg Steps 167.93 * Episodic Reward is ==> -165.91349185591463 * Lastest 100 Episods Avg Reward is ==> -161.03041092913773\n",
            "Episode * 1110 * Avg Steps 166.22 * Episodic Reward is ==> 59.63126929653747 * Lastest 100 Episods Avg Reward is ==> -152.99041182479598\n",
            "Episode * 1120 * Avg Steps 172.85 * Episodic Reward is ==> 70.33421615647376 * Lastest 100 Episods Avg Reward is ==> -140.2985183439631\n",
            "Episode * 1130 * Avg Steps 191.13 * Episodic Reward is ==> 28.379692724660256 * Lastest 100 Episods Avg Reward is ==> -124.23674039989749\n",
            "Episode * 1140 * Avg Steps 206.96 * Episodic Reward is ==> -27.342679728766868 * Lastest 100 Episods Avg Reward is ==> -107.40939889305848\n",
            "Episode * 1150 * Avg Steps 224.01 * Episodic Reward is ==> 19.611950764382104 * Lastest 100 Episods Avg Reward is ==> -91.57239759625992\n",
            "Episode * 1160 * Avg Steps 234.52 * Episodic Reward is ==> 43.952809779190474 * Lastest 100 Episods Avg Reward is ==> -83.36950722322696\n",
            "Episode * 1170 * Avg Steps 242.12 * Episodic Reward is ==> -254.4928135558912 * Lastest 100 Episods Avg Reward is ==> -68.90384414802708\n",
            "Episode * 1180 * Avg Steps 250.86 * Episodic Reward is ==> 27.475556735937623 * Lastest 100 Episods Avg Reward is ==> -51.01806041876447\n",
            "Episode * 1190 * Avg Steps 255.87 * Episodic Reward is ==> -8.640895406596094 * Lastest 100 Episods Avg Reward is ==> -24.258578678002344\n",
            "Episode * 1200 * Avg Steps 271.52 * Episodic Reward is ==> 25.216637682535413 * Lastest 100 Episods Avg Reward is ==> -7.493244187774509\n",
            "Episode * 1210 * Avg Steps 283.04 * Episodic Reward is ==> 81.03969363636563 * Lastest 100 Episods Avg Reward is ==> -3.921355897485388\n",
            "Episode * 1220 * Avg Steps 285.0 * Episodic Reward is ==> 51.128889369847805 * Lastest 100 Episods Avg Reward is ==> -5.178543091842261\n",
            "Episode * 1230 * Avg Steps 285.0 * Episodic Reward is ==> -3.0150430122754197 * Lastest 100 Episods Avg Reward is ==> -0.943984173752617\n",
            "Episode * 1240 * Avg Steps 288.2 * Episodic Reward is ==> 59.14405652440735 * Lastest 100 Episods Avg Reward is ==> 5.718550354700408\n",
            "Episode * 1250 * Avg Steps 287.75 * Episodic Reward is ==> -346.618937321317 * Lastest 100 Episods Avg Reward is ==> 7.4112496313532095\n",
            "Episode * 1260 * Avg Steps 281.65 * Episodic Reward is ==> -125.03431780151607 * Lastest 100 Episods Avg Reward is ==> -7.834834841570057\n",
            "Episode * 1270 * Avg Steps 276.15 * Episodic Reward is ==> 2.4704129637755092 * Lastest 100 Episods Avg Reward is ==> -18.727743143531388\n",
            "Episode * 1280 * Avg Steps 271.75 * Episodic Reward is ==> 19.743510366800606 * Lastest 100 Episods Avg Reward is ==> -23.426922449301813\n",
            "Episode * 1290 * Avg Steps 257.0 * Episodic Reward is ==> -36.1599646662375 * Lastest 100 Episods Avg Reward is ==> -29.135751296666626\n",
            "Episode * 1300 * Avg Steps 253.64 * Episodic Reward is ==> -3.7096776106169607 * Lastest 100 Episods Avg Reward is ==> -33.8929488151419\n",
            "Episode * 1310 * Avg Steps 250.35 * Episodic Reward is ==> -3.008251203304501 * Lastest 100 Episods Avg Reward is ==> -35.73394777216037\n",
            "Episode * 1320 * Avg Steps 237.99 * Episodic Reward is ==> -197.8646384114985 * Lastest 100 Episods Avg Reward is ==> -49.36649184291429\n",
            "Episode * 1330 * Avg Steps 235.45 * Episodic Reward is ==> -293.1586971871008 * Lastest 100 Episods Avg Reward is ==> -64.09824599367015\n",
            "Episode * 1340 * Avg Steps 234.37 * Episodic Reward is ==> -6.835476875993227 * Lastest 100 Episods Avg Reward is ==> -69.0100255661536\n",
            "Episode * 1350 * Avg Steps 235.37 * Episodic Reward is ==> 29.719196449677703 * Lastest 100 Episods Avg Reward is ==> -69.54603767653704\n",
            "Episode * 1360 * Avg Steps 230.83 * Episodic Reward is ==> -59.057490434453634 * Lastest 100 Episods Avg Reward is ==> -56.316197463900515\n",
            "Episode * 1370 * Avg Steps 233.06 * Episodic Reward is ==> 66.51412810456685 * Lastest 100 Episods Avg Reward is ==> -56.17449571857821\n",
            "Episode * 1380 * Avg Steps 237.46 * Episodic Reward is ==> 22.84924507288627 * Lastest 100 Episods Avg Reward is ==> -51.885444951553445\n",
            "Episode * 1390 * Avg Steps 238.83 * Episodic Reward is ==> 34.83881746481793 * Lastest 100 Episods Avg Reward is ==> -74.48313923818638\n",
            "Episode * 1400 * Avg Steps 232.08 * Episodic Reward is ==> -424.907903986274 * Lastest 100 Episods Avg Reward is ==> -86.3468920303568\n",
            "Episode * 1410 * Avg Steps 232.52 * Episodic Reward is ==> -18.751171852694597 * Lastest 100 Episods Avg Reward is ==> -95.36112185956398\n",
            "Episode * 1420 * Avg Steps 244.15 * Episodic Reward is ==> -321.4604401988926 * Lastest 100 Episods Avg Reward is ==> -86.57562331607036\n",
            "Episode * 1430 * Avg Steps 240.28 * Episodic Reward is ==> -353.3942067108956 * Lastest 100 Episods Avg Reward is ==> -78.31984066144186\n",
            "Episode * 1440 * Avg Steps 236.95 * Episodic Reward is ==> 31.481301844695057 * Lastest 100 Episods Avg Reward is ==> -70.92449724780539\n",
            "Episode * 1450 * Avg Steps 237.87 * Episodic Reward is ==> 10.56550616122783 * Lastest 100 Episods Avg Reward is ==> -72.28943063481697\n",
            "Episode * 1460 * Avg Steps 249.78 * Episodic Reward is ==> -38.1067040385137 * Lastest 100 Episods Avg Reward is ==> -76.11540573350926\n",
            "Episode * 1470 * Avg Steps 252.45 * Episodic Reward is ==> -275.4745596938984 * Lastest 100 Episods Avg Reward is ==> -73.31831806716869\n",
            "Episode * 1480 * Avg Steps 246.85 * Episodic Reward is ==> 28.444335263325957 * Lastest 100 Episods Avg Reward is ==> -86.31627112240767\n",
            "Episode * 1490 * Avg Steps 257.47 * Episodic Reward is ==> -142.91009325271213 * Lastest 100 Episods Avg Reward is ==> -65.04303309934737\n",
            "Episode * 1500 * Avg Steps 260.72 * Episodic Reward is ==> -68.12539541388897 * Lastest 100 Episods Avg Reward is ==> -62.157688227631205\n",
            "Episode * 1510 * Avg Steps 252.7 * Episodic Reward is ==> -249.09964786359546 * Lastest 100 Episods Avg Reward is ==> -70.58546202023423\n",
            "Episode * 1520 * Avg Steps 239.4 * Episodic Reward is ==> -428.84843859900593 * Lastest 100 Episods Avg Reward is ==> -95.69828602372647\n",
            "Episode * 1530 * Avg Steps 232.67 * Episodic Reward is ==> -131.7681791916063 * Lastest 100 Episods Avg Reward is ==> -115.56854033416629\n",
            "Episode * 1540 * Avg Steps 229.55 * Episodic Reward is ==> -149.86171251520148 * Lastest 100 Episods Avg Reward is ==> -123.68019259709827\n",
            "Episode * 1550 * Avg Steps 210.98 * Episodic Reward is ==> -101.54243981798324 * Lastest 100 Episods Avg Reward is ==> -134.85360069683057\n",
            "Episode * 1560 * Avg Steps 197.15 * Episodic Reward is ==> -335.35889976727987 * Lastest 100 Episods Avg Reward is ==> -146.16517333332342\n",
            "Episode * 1570 * Avg Steps 186.43 * Episodic Reward is ==> -289.67022083951883 * Lastest 100 Episods Avg Reward is ==> -157.51833674261817\n",
            "Episode * 1580 * Avg Steps 180.92 * Episodic Reward is ==> -80.509682777038 * Lastest 100 Episods Avg Reward is ==> -172.74032532111514\n",
            "Episode * 1590 * Avg Steps 176.25 * Episodic Reward is ==> 60.790835858951766 * Lastest 100 Episods Avg Reward is ==> -181.7635519603825\n",
            "Episode * 1600 * Avg Steps 171.21 * Episodic Reward is ==> 15.326579236057883 * Lastest 100 Episods Avg Reward is ==> -179.25118894389365\n",
            "Episode * 1610 * Avg Steps 169.67 * Episodic Reward is ==> -168.21923294387884 * Lastest 100 Episods Avg Reward is ==> -176.49840050989275\n",
            "Episode * 1620 * Avg Steps 175.11 * Episodic Reward is ==> -217.53418669518737 * Lastest 100 Episods Avg Reward is ==> -160.45735760861132\n",
            "Episode * 1630 * Avg Steps 173.46 * Episodic Reward is ==> -54.697436111453534 * Lastest 100 Episods Avg Reward is ==> -151.11148091197379\n",
            "Episode * 1640 * Avg Steps 168.85 * Episodic Reward is ==> 45.953210682957355 * Lastest 100 Episods Avg Reward is ==> -155.30623699147438\n",
            "Episode * 1650 * Avg Steps 175.69 * Episodic Reward is ==> 107.61884186594393 * Lastest 100 Episods Avg Reward is ==> -153.95559337557526\n",
            "Episode * 1660 * Avg Steps 179.21 * Episodic Reward is ==> -174.99411217994106 * Lastest 100 Episods Avg Reward is ==> -152.36632727098902\n",
            "Episode * 1670 * Avg Steps 185.29 * Episodic Reward is ==> -130.46011014520602 * Lastest 100 Episods Avg Reward is ==> -138.75100276942416\n",
            "Episode * 1680 * Avg Steps 192.1 * Episodic Reward is ==> -111.22679983846808 * Lastest 100 Episods Avg Reward is ==> -130.5417153812031\n",
            "Episode * 1690 * Avg Steps 190.24 * Episodic Reward is ==> 56.939773519046554 * Lastest 100 Episods Avg Reward is ==> -130.53411274884988\n",
            "Episode * 1700 * Avg Steps 190.11 * Episodic Reward is ==> -182.25740884543683 * Lastest 100 Episods Avg Reward is ==> -135.82192694306602\n",
            "Episode * 1710 * Avg Steps 194.88 * Episodic Reward is ==> -226.62361565079215 * Lastest 100 Episods Avg Reward is ==> -131.43986982235657\n",
            "Episode * 1720 * Avg Steps 191.87 * Episodic Reward is ==> -78.76881690476611 * Lastest 100 Episods Avg Reward is ==> -129.1324764128719\n",
            "Episode * 1730 * Avg Steps 198.5 * Episodic Reward is ==> -119.83157057227518 * Lastest 100 Episods Avg Reward is ==> -131.73236781908557\n",
            "Episode * 1740 * Avg Steps 207.88 * Episodic Reward is ==> -22.73092978212817 * Lastest 100 Episods Avg Reward is ==> -133.31164604878342\n",
            "Episode * 1750 * Avg Steps 210.74 * Episodic Reward is ==> 51.79275297949594 * Lastest 100 Episods Avg Reward is ==> -135.61080164171887\n",
            "Episode * 1760 * Avg Steps 208.63 * Episodic Reward is ==> 34.76102974478022 * Lastest 100 Episods Avg Reward is ==> -135.82685148950446\n",
            "Episode * 1770 * Avg Steps 215.11 * Episodic Reward is ==> 38.234408066851934 * Lastest 100 Episods Avg Reward is ==> -131.90299352972275\n",
            "Episode * 1780 * Avg Steps 213.57 * Episodic Reward is ==> 26.266370290363984 * Lastest 100 Episods Avg Reward is ==> -124.78805048299989\n",
            "Episode * 1790 * Avg Steps 221.32 * Episodic Reward is ==> -110.97807358617732 * Lastest 100 Episods Avg Reward is ==> -109.77081599682887\n",
            "Episode * 1800 * Avg Steps 229.01 * Episodic Reward is ==> -178.98468471135027 * Lastest 100 Episods Avg Reward is ==> -120.84057420679703\n",
            "Episode * 1810 * Avg Steps 236.96 * Episodic Reward is ==> -10.955187698275036 * Lastest 100 Episods Avg Reward is ==> -107.45368424121075\n",
            "Episode * 1820 * Avg Steps 235.24 * Episodic Reward is ==> -73.50671595995789 * Lastest 100 Episods Avg Reward is ==> -107.76501444559125\n",
            "Episode * 1830 * Avg Steps 230.07 * Episodic Reward is ==> -86.52604743158666 * Lastest 100 Episods Avg Reward is ==> -94.53135812157564\n",
            "Episode * 1840 * Avg Steps 231.5 * Episodic Reward is ==> 122.2997791827776 * Lastest 100 Episods Avg Reward is ==> -81.57361610759443\n",
            "Episode * 1850 * Avg Steps 223.8 * Episodic Reward is ==> -115.17090043616965 * Lastest 100 Episods Avg Reward is ==> -74.50357150249165\n",
            "Episode * 1860 * Avg Steps 229.58 * Episodic Reward is ==> 114.76516974010433 * Lastest 100 Episods Avg Reward is ==> -56.92033667334178\n",
            "Episode * 1870 * Avg Steps 224.99 * Episodic Reward is ==> 99.03823708497441 * Lastest 100 Episods Avg Reward is ==> -60.07194449265312\n",
            "Episode * 1880 * Avg Steps 230.83 * Episodic Reward is ==> 107.23206488497215 * Lastest 100 Episods Avg Reward is ==> -46.863489308608926\n",
            "Episode * 1890 * Avg Steps 233.83 * Episodic Reward is ==> 87.56893324222736 * Lastest 100 Episods Avg Reward is ==> -37.477734069568555\n",
            "Episode * 1900 * Avg Steps 236.0 * Episodic Reward is ==> 85.89187761619235 * Lastest 100 Episods Avg Reward is ==> -10.73830778335344\n",
            "Episode * 1910 * Avg Steps 233.1 * Episodic Reward is ==> 49.584808197928545 * Lastest 100 Episods Avg Reward is ==> -19.21705326424272\n",
            "Episode * 1920 * Avg Steps 235.75 * Episodic Reward is ==> -428.7400581374646 * Lastest 100 Episods Avg Reward is ==> -35.93966287120084\n",
            "Episode * 1930 * Avg Steps 243.25 * Episodic Reward is ==> -190.9090029427403 * Lastest 100 Episods Avg Reward is ==> -42.712841716408214\n",
            "Episode * 1940 * Avg Steps 242.45 * Episodic Reward is ==> -364.2217666662795 * Lastest 100 Episods Avg Reward is ==> -55.15255715460134\n",
            "Episode * 1950 * Avg Steps 256.52 * Episodic Reward is ==> 85.24466438293425 * Lastest 100 Episods Avg Reward is ==> -50.36587067163919\n",
            "Episode * 1960 * Avg Steps 252.28 * Episodic Reward is ==> 109.1179725052762 * Lastest 100 Episods Avg Reward is ==> -69.7838886762686\n",
            "Episode * 1970 * Avg Steps 256.9 * Episodic Reward is ==> -687.0149190944201 * Lastest 100 Episods Avg Reward is ==> -66.9786221754724\n",
            "Episode * 1980 * Avg Steps 256.9 * Episodic Reward is ==> 27.990174325232992 * Lastest 100 Episods Avg Reward is ==> -66.83878467272282\n",
            "Episode * 1990 * Avg Steps 255.78 * Episodic Reward is ==> 189.3416233493706 * Lastest 100 Episods Avg Reward is ==> -80.64254467393124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-abe236c59510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-29400ee006e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_episodes, max_steps)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ep_reward\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mep_reward_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"avg_reward\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mavg_reward_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mep_steps_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ddpg_per_{self.per}_{date_time}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ep_reward_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuF4rIYqwyZ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}