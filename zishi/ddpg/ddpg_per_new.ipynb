{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg_per_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otdpa1QZV19H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1bb7293-b30e-44f9-adaf-7ae4a67754b1"
      },
      "source": [
        "!pip install --upgrade --force-reinstall box2d-py\n",
        "!pip install --upgrade --force-reinstall gym[Box_2D]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 13.4MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 7.7MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting gym[Box_2D]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 8.9MB/s \n",
            "\u001b[33m  WARNING: gym 0.18.0 does not provide the extra 'box_2d'\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/91/ee427c42957f8c4cbe477bf4f8b7f608e003a17941e509d1777e58648cb3/scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4MB 111kB/s \n",
            "\u001b[?25hCollecting numpy>=1.10.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 119kB/s \n",
            "\u001b[?25hCollecting pyglet<=1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.0MB/s \n",
            "\u001b[?25hCollecting Pillow<=7.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 49.6MB/s \n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Collecting future\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 49.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.18.0-cp37-none-any.whl size=1656451 sha256=c63cfcc29c62661c9aaac22cf4999ff3061068565c4a3cdb6afd6b830a054b93\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=0e5d5393ec17b8d2c2fa110dc469bf7d4d96dada2775a82dfd6ec828cc26d2ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built gym future\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scipy, future, pyglet, Pillow, cloudpickle, gym\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 future-0.18.2 gym-0.18.0 numpy-1.20.2 pyglet-1.5.0 scipy-1.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpaMSP_LwgQp"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNXpnq0jt4uO"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,activations\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "RANDOM_SEEDS = 123\n",
        "\n",
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZi8mrsrt9HJ"
      },
      "source": [
        "# Ornstein-Uhlenbeck process\n",
        "class OUNoise:\n",
        "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = (self.x_prev \n",
        "            + self.theta * (self.mu - self.x_prev) * self.dt \n",
        "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape))\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x_initial if self.x_initial is not None else np.zeros_like(self.mu)\n",
        "\n",
        "class SumTree(object):\n",
        "    \"\"\"\n",
        "    This SumTree code is a modified version and the original code is from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
        "    Story data with its priority in the tree.\n",
        "    \"\"\"\n",
        "    data_pointer = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity  # for all priority values\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
        "        #             size: capacity - 1                       size: capacity\n",
        "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
        "        # [--------------data frame-------------]\n",
        "        #             size: capacity\n",
        "\n",
        "    def add(self, p, data):\n",
        "        tree_idx = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data  # update data_frame\n",
        "        self.update(tree_idx, p)  # update tree_frame\n",
        "\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_idx, p):\n",
        "        change = p - self.tree[tree_idx]\n",
        "        self.tree[tree_idx] = p\n",
        "        # then propagate the change through tree\n",
        "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
        "            tree_idx = (tree_idx - 1) // 2\n",
        "            self.tree[tree_idx] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"\n",
        "        Tree structure and array storage:\n",
        "        Tree index:\n",
        "             0         -> storing priority sum\n",
        "            / \\\n",
        "          1     2\n",
        "         / \\   / \\\n",
        "        3   4 5   6    -> storing priority for transitions\n",
        "        Array type for storing:\n",
        "        [0,1,2,3,4,5,6]\n",
        "        \"\"\"\n",
        "        parent_idx = 0\n",
        "        while True:     # the while loop is faster than the method in the reference code\n",
        "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
        "            cr_idx = cl_idx + 1\n",
        "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
        "                leaf_idx = parent_idx\n",
        "                break\n",
        "            else:       # downward search, always search for a higher priority node\n",
        "                if v <= self.tree[cl_idx]:\n",
        "                    parent_idx = cl_idx\n",
        "                else:\n",
        "                    v -= self.tree[cl_idx]\n",
        "                    parent_idx = cr_idx\n",
        "\n",
        "        data_idx = leaf_idx - self.capacity + 1\n",
        "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
        "\n",
        "    @property\n",
        "    def total_p(self):\n",
        "        return self.tree[0]  # the root\n",
        "\n",
        "\n",
        "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    \"\"\"\n",
        "    This Memory class is modified based on the original code from:\n",
        "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
        "    \"\"\"\n",
        "    epsilon = 0.01  # small amount to avoid zero priority\n",
        "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
        "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
        "    beta_increment_per_sampling = 0.001\n",
        "    abs_err_upper = 1.  # clipped abs error\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.sample_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sample_count\n",
        "\n",
        "    def store(self, transition):\n",
        "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
        "        if max_p == 0:\n",
        "            max_p = self.abs_err_upper\n",
        "        self.tree.add(max_p, transition)   # set the max p for new p\n",
        "        self.sample_count = min(self.sample_count + 1, self.tree.capacity)\n",
        "\n",
        "    def sample(self, n):\n",
        "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
        "        pri_seg = self.tree.total_p / n       # priority segment\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
        "\n",
        "        a = self.tree.tree[-self.tree.capacity:]\n",
        "        min_prob = np.min(a[a != 0]) / self.tree.total_p     # for later calculate ISweight\n",
        "        for i in range(n):\n",
        "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
        "            v = np.random.uniform(a, b)\n",
        "            idx, p, data = self.tree.get_leaf(v)\n",
        "            prob = p / self.tree.total_p\n",
        "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
        "            b_idx[i], b_memory[i, :] = idx, data\n",
        "\n",
        "        return b_idx, b_memory, ISWeights\n",
        "\n",
        "    def batch_update(self, tree_idx, abs_errors):\n",
        "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
        "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
        "        ps = np.power(clipped_errors, self.alpha)\n",
        "        for ti, p in zip(tree_idx, ps):\n",
        "            self.tree.update(ti, p)\n",
        "\n",
        "\n",
        "def get_actor(state_shape, action_dim, upper_bound, units=(512,512)):\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.004, maxval=0.004)\n",
        "    inputs = layers.Input(shape=(state_shape,))\n",
        "    for idx, unit in enumerate(units):\n",
        "        if idx == 0:\n",
        "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(inputs)\n",
        "        else:\n",
        "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(action_dim, name=\"Output\", activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "    scaled_outputs = outputs * upper_bound\n",
        "    model = Model(inputs,scaled_outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic(state_shape, action_dim, state_units=(512,256),action_units=128,concat_units=128):\n",
        "    state_input = layers.Input(shape=(state_shape), name=\"state_input\")\n",
        "    for idx, unit in enumerate(state_units):      \n",
        "        if idx == 0:\n",
        "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_input)\n",
        "        else:\n",
        "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_out)\n",
        "    \n",
        "    action_input = layers.Input(shape=(action_dim),name=\"action_input\")\n",
        "    action_out = layers.Dense(action_units, name=\"action_lvl\", activation=\"relu\")(action_input)\n",
        "    \n",
        "    concat = layers.Concatenate()([state_out,action_out])\n",
        "    out = layers.Dense(concat_units, name=\"concat_lvl\",activation=\"relu\")(concat)\n",
        "    outputs = layers.Dense(1)(out) # Q value\n",
        "\n",
        "    model = Model([state_input, action_input], outputs)\n",
        "    return model\n",
        "\n",
        "@tf.function\n",
        "def update_target(model, target_model, tau=0.001):\n",
        "    weights = model.variables\n",
        "    target_weights = target_model.variables\n",
        "    for (a,b) in zip(target_weights,weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP_5PWKuhj6T"
      },
      "source": [
        "class DDPG:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 per=False,\n",
        "                 actor_lr=0.00005,\n",
        "                 critic_lr=0.0005,\n",
        "                 actor_units=(512,256),\n",
        "                 state_units=(512,256),\n",
        "                 action_units=128,\n",
        "                 concat_units=128,\n",
        "                 noise=\"OU\",\n",
        "                 tau=0.001,\n",
        "                 gamma=0.99,\n",
        "                 batch_size=64,\n",
        "                 memory_size=2**17\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.per = per\n",
        "        self.state_shape = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.upper_bound = env.action_space.high[0]\n",
        "        self.lower_bound = env.action_space.low[0]\n",
        "        if noise == \"OU\":\n",
        "            self.noise = OUNoise(mu=np.zeros(self.action_dim),sigma=float(0.2)*np.ones(self.action_dim))\n",
        "        else:\n",
        "            self.noise = np.random.normal(scale=0.2, size=self.action_dim)\n",
        "        self.actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
        "        self.critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
        "        self.target_actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
        "        self.target_critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
        "        self.target_actor_model.set_weights(self.actor_model.get_weights())\n",
        "        self.target_critic_model.set_weights(self.critic_model.get_weights())\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = Memory(capacity=memory_size) if per else deque(maxlen=memory_size)\n",
        "        self.alpha_decay = 0.999\n",
        "\n",
        "    def policy(self, state, noise_object):\n",
        "        sampled_actions = tf.squeeze(self.actor_model(state))\n",
        "\n",
        "        noise = noise_object()\n",
        "        #sampled_actions = sampled_actions.numpy() + noise\n",
        "        sampled_actions = sampled_actions.numpy()\n",
        "        legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
        "        return np.squeeze(legal_action)\n",
        "\n",
        "    def record(self, obs_array):\n",
        "        if self.per:\n",
        "            transition = np.hstack(obs_array)\n",
        "            self.memory.store(transition)\n",
        "        else:\n",
        "            self.memory.append(obs_array)    \n",
        "\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def per_update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights\n",
        "    ):\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
        "            y = reward_batch + self.gamma * self.target_critic_model(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            ) * (1. - done_batch)\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            errors = y - critic_value\n",
        "            critic_loss = tf.math.reduce_mean(ISWeights * tf.math.square(errors))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, self.critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        abs_errors = tf.reduce_sum(tf.abs(errors), axis=1)\n",
        "        #self.memory.batch_update(tree_idx, abs_errors)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor_model.trainable_variables)\n",
        "        )\n",
        "        return abs_errors\n",
        "\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "    ):\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
        "            y = reward_batch + self.gamma * self.target_critic_model(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            ) * (1. - done_batch)\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.keras.losses.Huber()(y,critic_value)\n",
        "\n",
        "            critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "            self.critic_optimizer.apply_gradients(\n",
        "                zip(critic_grad, self.critic_model.trainable_variables)\n",
        "            )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        if self.per:\n",
        "            tree_idx, samples, ISWeights = self.memory.sample(min(self.batch_size,len(self.memory)))\n",
        "            ISWeights_batch = tf.convert_to_tensor(ISWeights,dtype=tf.float32)\n",
        "            split_shape = np.cumsum([self.state_shape, self.action_dim, 1, self.state_shape])\n",
        "            states, actions, rewards, next_states, dones = np.hsplit(samples, split_shape)\n",
        "            state_batch = tf.convert_to_tensor(states)\n",
        "            action_batch = tf.convert_to_tensor(actions)\n",
        "            reward_batch = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(next_states)\n",
        "            done_batch = tf.convert_to_tensor(dones,dtype=tf.float32)\n",
        "            abs_errors = self.per_update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights_batch)\n",
        "            self.memory.batch_update(tree_idx, abs_errors.numpy())\n",
        "        else:\n",
        "            samples = random.sample(self.memory, min(self.batch_size,len(self.memory)))\n",
        "            trans_s = np.array(samples,dtype=object).T\n",
        "            state_batch = tf.convert_to_tensor(np.row_stack(trans_s[0]))\n",
        "            action_batch = tf.convert_to_tensor(np.row_stack(trans_s[1]))\n",
        "            reward_batch = tf.convert_to_tensor(np.row_stack(trans_s[2]),dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(np.row_stack(trans_s[3]))\n",
        "            done_batch = tf.convert_to_tensor(np.row_stack(trans_s[4]),dtype=tf.float32)\n",
        "            self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "    def train(self, max_episodes=2000, max_steps=1000):\n",
        "        self.ep_reward_list = []\n",
        "        self.avg_reward_list = []\n",
        "        self.ep_steps_list = []\n",
        "\n",
        "        for ep in range(max_episodes):\n",
        "            prev_state = self.env.reset()\n",
        "            episodic_reward = 0\n",
        "            steps = 0\n",
        "            \n",
        "            while steps < max_steps:\n",
        "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "                action = self.policy(tf_prev_state, self.noise)\n",
        "\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                self.record([prev_state, action, reward, state, done])\n",
        "\n",
        "                episodic_reward += reward\n",
        "                self.replay()\n",
        "\n",
        "                update_target(self.actor_model, self.target_actor_model, self.tau)\n",
        "                update_target(self.critic_model, self.target_critic_model, self.tau)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                prev_state = state\n",
        "                steps += 1\n",
        "            #if self.per == True:\n",
        "            #    self.memory.alpha *= self.alpha_decay\n",
        "            self.ep_reward_list.append(episodic_reward)\n",
        "            # Mean of last 100 episodes\n",
        "            avg_reward = np.mean(self.ep_reward_list[-100:])\n",
        "            self.avg_reward_list.append(avg_reward)\n",
        "            self.ep_steps_list.append(steps)\n",
        "            avg_step = np.mean(self.ep_steps_list[-100:])\n",
        "            if ep%10 == 0:\n",
        "                print(f\"Episode * {ep} * Avg Steps {avg_step} * Episodic Reward is ==> {episodic_reward} * Lastest 100 Episods Avg Reward is ==> {avg_reward}\")\n",
        "\n",
        "\n",
        "        plt.plot(self.avg_reward_list)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "        plt.show()\n",
        "\n",
        "        content = {\"ep_reward\":self.ep_reward_list,\"avg_reward\":self.avg_reward_list, \"steps\":self.ep_steps_list}\n",
        "        df = pd.DataFrame(content)\n",
        "        df.to_csv(f'ddpg_per_{self.per}_{date_time}.csv') \n",
        "        files.download(f'ddpg_per_{self.per}_{date_time}.csv')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmmC-HiYh16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c00ce64-8293-48f9-f897-189c83fc480e"
      },
      "source": [
        "problem = 'LunarLanderContinuous-v2'\n",
        "gym_env = gym.make(problem)\n",
        "gym_env.seed(RANDOM_SEEDS)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLkJwfK0hvvw"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "ddpg = DDPG(gym_env,per=True)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fb2tBUrvjynI",
        "outputId": "7ad77f52-2e45-49f6-e926-e105013f2b57"
      },
      "source": [
        "ddpg.train(max_episodes=2000,max_steps=1000)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Avg Steps 112.0 * Episodic Reward is ==> -42.15727939931744 * Lastest 100 Episods Avg Reward is ==> -42.15727939931744\n",
            "Episode * 10 * Avg Steps 75.0 * Episodic Reward is ==> -492.56183600409486 * Lastest 100 Episods Avg Reward is ==> -447.77187526412393\n",
            "Episode * 20 * Avg Steps 73.0952380952381 * Episodic Reward is ==> -159.50763354094565 * Lastest 100 Episods Avg Reward is ==> -433.941097300297\n",
            "Episode * 30 * Avg Steps 71.58064516129032 * Episodic Reward is ==> -145.09849792097145 * Lastest 100 Episods Avg Reward is ==> -389.5839819268459\n",
            "Episode * 40 * Avg Steps 68.78048780487805 * Episodic Reward is ==> -110.93648866176594 * Lastest 100 Episods Avg Reward is ==> -324.17890205715537\n",
            "Episode * 50 * Avg Steps 68.68627450980392 * Episodic Reward is ==> -96.15483064552824 * Lastest 100 Episods Avg Reward is ==> -284.4047013622693\n",
            "Episode * 60 * Avg Steps 68.8360655737705 * Episodic Reward is ==> -160.73619094822803 * Lastest 100 Episods Avg Reward is ==> -260.59231598969063\n",
            "Episode * 70 * Avg Steps 68.45070422535211 * Episodic Reward is ==> -165.93308284939968 * Lastest 100 Episods Avg Reward is ==> -243.95724288182114\n",
            "Episode * 80 * Avg Steps 69.4320987654321 * Episodic Reward is ==> -433.61590509918847 * Lastest 100 Episods Avg Reward is ==> -236.27711810423182\n",
            "Episode * 90 * Avg Steps 70.04395604395604 * Episodic Reward is ==> -99.98610194393528 * Lastest 100 Episods Avg Reward is ==> -224.60497435653272\n",
            "Episode * 100 * Avg Steps 69.24 * Episodic Reward is ==> -124.65856210813882 * Lastest 100 Episods Avg Reward is ==> -217.8185543089107\n",
            "Episode * 110 * Avg Steps 68.85 * Episodic Reward is ==> -110.81789272499338 * Lastest 100 Episods Avg Reward is ==> -182.92507098317233\n",
            "Episode * 120 * Avg Steps 68.96 * Episodic Reward is ==> -173.6133550523591 * Lastest 100 Episods Avg Reward is ==> -154.46062204111496\n",
            "Episode * 130 * Avg Steps 68.96 * Episodic Reward is ==> -140.82805237984422 * Lastest 100 Episods Avg Reward is ==> -138.29103833660156\n",
            "Episode * 140 * Avg Steps 69.32 * Episodic Reward is ==> -167.42971915062216 * Lastest 100 Episods Avg Reward is ==> -139.74627537593332\n",
            "Episode * 150 * Avg Steps 69.28 * Episodic Reward is ==> -62.57238219827897 * Lastest 100 Episods Avg Reward is ==> -140.23693708187182\n",
            "Episode * 160 * Avg Steps 75.84 * Episodic Reward is ==> -477.6771775387714 * Lastest 100 Episods Avg Reward is ==> -173.88508828539204\n",
            "Episode * 170 * Avg Steps 77.92 * Episodic Reward is ==> -417.49053535344336 * Lastest 100 Episods Avg Reward is ==> -209.69810665014552\n",
            "Episode * 180 * Avg Steps 93.05 * Episodic Reward is ==> -458.00794344308366 * Lastest 100 Episods Avg Reward is ==> -217.98032936694597\n",
            "Episode * 190 * Avg Steps 102.42 * Episodic Reward is ==> -204.39379447576334 * Lastest 100 Episods Avg Reward is ==> -249.76844934568453\n",
            "Episode * 200 * Avg Steps 111.64 * Episodic Reward is ==> -190.60798537890435 * Lastest 100 Episods Avg Reward is ==> -274.9431843741793\n",
            "Episode * 210 * Avg Steps 129.75 * Episodic Reward is ==> -140.52477622672131 * Lastest 100 Episods Avg Reward is ==> -281.15291028186397\n",
            "Episode * 220 * Avg Steps 138.2 * Episodic Reward is ==> -254.32841314997006 * Lastest 100 Episods Avg Reward is ==> -283.88271131974113\n",
            "Episode * 230 * Avg Steps 165.85 * Episodic Reward is ==> -157.43886485260975 * Lastest 100 Episods Avg Reward is ==> -289.9473705396524\n",
            "Episode * 240 * Avg Steps 248.54 * Episodic Reward is ==> -84.86340425264967 * Lastest 100 Episods Avg Reward is ==> -288.3390180682205\n",
            "Episode * 250 * Avg Steps 330.09 * Episodic Reward is ==> -85.40727061209805 * Lastest 100 Episods Avg Reward is ==> -286.1118528940504\n",
            "Episode * 260 * Avg Steps 416.47 * Episodic Reward is ==> -46.24623994430394 * Lastest 100 Episods Avg Reward is ==> -245.84626858032306\n",
            "Episode * 270 * Avg Steps 500.0 * Episodic Reward is ==> -16.418784035791926 * Lastest 100 Episods Avg Reward is ==> -201.5328713176295\n",
            "Episode * 280 * Avg Steps 577.13 * Episodic Reward is ==> -61.200150401173 * Lastest 100 Episods Avg Reward is ==> -182.50086588309622\n",
            "Episode * 290 * Avg Steps 660.16 * Episodic Reward is ==> -94.57425235515419 * Lastest 100 Episods Avg Reward is ==> -145.5002514978676\n",
            "Episode * 300 * Avg Steps 744.22 * Episodic Reward is ==> -86.08728013276134 * Lastest 100 Episods Avg Reward is ==> -113.10416343534843\n",
            "Episode * 310 * Avg Steps 819.27 * Episodic Reward is ==> -58.86341602765851 * Lastest 100 Episods Avg Reward is ==> -98.30411859175798\n",
            "Episode * 320 * Avg Steps 903.51 * Episodic Reward is ==> -56.9059619079099 * Lastest 100 Episods Avg Reward is ==> -86.60697251639459\n",
            "Episode * 330 * Avg Steps 968.92 * Episodic Reward is ==> -8.186333428063538 * Lastest 100 Episods Avg Reward is ==> -70.75839941332048\n",
            "Episode * 340 * Avg Steps 979.76 * Episodic Reward is ==> -17.849053187631124 * Lastest 100 Episods Avg Reward is ==> -59.558697876627406\n",
            "Episode * 350 * Avg Steps 991.32 * Episodic Reward is ==> -20.141692504479845 * Lastest 100 Episods Avg Reward is ==> -51.14789512658455\n",
            "Episode * 360 * Avg Steps 991.32 * Episodic Reward is ==> -23.670876651423566 * Lastest 100 Episods Avg Reward is ==> -45.259155053071424\n",
            "Episode * 370 * Avg Steps 999.0 * Episodic Reward is ==> 23.68389945372714 * Lastest 100 Episods Avg Reward is ==> -40.12480760293425\n",
            "Episode * 380 * Avg Steps 999.0 * Episodic Reward is ==> -16.592343455973694 * Lastest 100 Episods Avg Reward is ==> -32.784842862694035\n",
            "Episode * 390 * Avg Steps 982.65 * Episodic Reward is ==> -42.601671318000776 * Lastest 100 Episods Avg Reward is ==> -31.139135704247796\n",
            "Episode * 400 * Avg Steps 979.39 * Episodic Reward is ==> -64.55928368634095 * Lastest 100 Episods Avg Reward is ==> -32.181858273397\n",
            "Episode * 410 * Avg Steps 979.39 * Episodic Reward is ==> 14.006697530592831 * Lastest 100 Episods Avg Reward is ==> -31.72142578695525\n",
            "Episode * 420 * Avg Steps 965.53 * Episodic Reward is ==> -38.41247093176064 * Lastest 100 Episods Avg Reward is ==> -32.65343288584543\n",
            "Episode * 430 * Avg Steps 952.27 * Episodic Reward is ==> -18.845585337536274 * Lastest 100 Episods Avg Reward is ==> -31.924789003339697\n",
            "Episode * 440 * Avg Steps 936.59 * Episodic Reward is ==> -67.91621585033491 * Lastest 100 Episods Avg Reward is ==> -35.12787538239963\n",
            "Episode * 450 * Avg Steps 928.26 * Episodic Reward is ==> -13.023023818295561 * Lastest 100 Episods Avg Reward is ==> -35.16087924767077\n",
            "Episode * 460 * Avg Steps 928.26 * Episodic Reward is ==> 19.759874367772614 * Lastest 100 Episods Avg Reward is ==> -35.94269983705012\n",
            "Episode * 470 * Avg Steps 914.84 * Episodic Reward is ==> 48.014500291139925 * Lastest 100 Episods Avg Reward is ==> -34.84641848197666\n",
            "Episode * 480 * Avg Steps 909.01 * Episodic Reward is ==> -27.798133859929457 * Lastest 100 Episods Avg Reward is ==> -37.303559209836855\n",
            "Episode * 490 * Avg Steps 901.71 * Episodic Reward is ==> 192.5689737762993 * Lastest 100 Episods Avg Reward is ==> -24.549130532185014\n",
            "Episode * 500 * Avg Steps 868.39 * Episodic Reward is ==> 255.27821738682667 * Lastest 100 Episods Avg Reward is ==> -3.9254789867695594\n",
            "Episode * 510 * Avg Steps 836.83 * Episodic Reward is ==> 171.11734044898964 * Lastest 100 Episods Avg Reward is ==> 21.016777245598863\n",
            "Episode * 520 * Avg Steps 820.1 * Episodic Reward is ==> 15.357069255034972 * Lastest 100 Episods Avg Reward is ==> 33.20687212439451\n",
            "Episode * 530 * Avg Steps 795.98 * Episodic Reward is ==> -71.65391058516494 * Lastest 100 Episods Avg Reward is ==> 39.34597846909514\n",
            "Episode * 540 * Avg Steps 794.92 * Episodic Reward is ==> 238.26419526018537 * Lastest 100 Episods Avg Reward is ==> 46.934445608096595\n",
            "Episode * 550 * Avg Steps 778.88 * Episodic Reward is ==> -284.2450077881678 * Lastest 100 Episods Avg Reward is ==> 52.884908805203295\n",
            "Episode * 560 * Avg Steps 733.2 * Episodic Reward is ==> -31.993689886197632 * Lastest 100 Episods Avg Reward is ==> 50.505148766038246\n",
            "Episode * 570 * Avg Steps 665.71 * Episodic Reward is ==> -205.8017799906794 * Lastest 100 Episods Avg Reward is ==> 31.099021571499694\n",
            "Episode * 580 * Avg Steps 607.05 * Episodic Reward is ==> -473.88236864760387 * Lastest 100 Episods Avg Reward is ==> 4.808028750556533\n",
            "Episode * 590 * Avg Steps 567.91 * Episodic Reward is ==> 16.487252155969784 * Lastest 100 Episods Avg Reward is ==> -26.356707120774125\n",
            "Episode * 600 * Avg Steps 566.03 * Episodic Reward is ==> 192.2931102460463 * Lastest 100 Episods Avg Reward is ==> -22.389357144799998\n",
            "Episode * 610 * Avg Steps 539.19 * Episodic Reward is ==> 236.33485984267324 * Lastest 100 Episods Avg Reward is ==> -19.49867426986573\n",
            "Episode * 620 * Avg Steps 513.18 * Episodic Reward is ==> 262.28128484185584 * Lastest 100 Episods Avg Reward is ==> -6.019338801824295\n",
            "Episode * 630 * Avg Steps 491.4 * Episodic Reward is ==> 196.2072369511191 * Lastest 100 Episods Avg Reward is ==> 13.676291075572982\n",
            "Episode * 640 * Avg Steps 441.94 * Episodic Reward is ==> 267.97175934708514 * Lastest 100 Episods Avg Reward is ==> 35.10450732633131\n",
            "Episode * 650 * Avg Steps 395.15 * Episodic Reward is ==> 237.1540010681777 * Lastest 100 Episods Avg Reward is ==> 57.485161572863426\n",
            "Episode * 660 * Avg Steps 367.88 * Episodic Reward is ==> 284.28571490126535 * Lastest 100 Episods Avg Reward is ==> 83.84985197597389\n",
            "Episode * 670 * Avg Steps 372.59 * Episodic Reward is ==> 260.17747108950977 * Lastest 100 Episods Avg Reward is ==> 129.29635652404073\n",
            "Episode * 680 * Avg Steps 365.51 * Episodic Reward is ==> 255.2249329313225 * Lastest 100 Episods Avg Reward is ==> 184.95424927146416\n",
            "Episode * 690 * Avg Steps 350.49 * Episodic Reward is ==> -268.13174584925207 * Lastest 100 Episods Avg Reward is ==> 231.16221954888218\n",
            "Episode * 700 * Avg Steps 310.96 * Episodic Reward is ==> 285.8768690926564 * Lastest 100 Episods Avg Reward is ==> 241.74533940035195\n",
            "Episode * 710 * Avg Steps 293.72 * Episodic Reward is ==> 272.3356497526174 * Lastest 100 Episods Avg Reward is ==> 245.31495060957405\n",
            "Episode * 720 * Avg Steps 274.1 * Episodic Reward is ==> 290.0975499716967 * Lastest 100 Episods Avg Reward is ==> 252.81156664511968\n",
            "Episode * 730 * Avg Steps 256.35 * Episodic Reward is ==> -197.86718235626472 * Lastest 100 Episods Avg Reward is ==> 249.13809863407448\n",
            "Episode * 740 * Avg Steps 260.86 * Episodic Reward is ==> 299.4658861474668 * Lastest 100 Episods Avg Reward is ==> 238.5217898816974\n",
            "Episode * 750 * Avg Steps 255.88 * Episodic Reward is ==> -235.22660906929636 * Lastest 100 Episods Avg Reward is ==> 223.82935801768284\n",
            "Episode * 760 * Avg Steps 257.03 * Episodic Reward is ==> 253.73107347559701 * Lastest 100 Episods Avg Reward is ==> 190.66610179222087\n",
            "Episode * 770 * Avg Steps 260.27 * Episodic Reward is ==> -143.51097386393286 * Lastest 100 Episods Avg Reward is ==> 156.86070590986037\n",
            "Episode * 780 * Avg Steps 261.1 * Episodic Reward is ==> -65.49487848797105 * Lastest 100 Episods Avg Reward is ==> 129.66365644439557\n",
            "Episode * 790 * Avg Steps 266.04 * Episodic Reward is ==> 210.17427484851362 * Lastest 100 Episods Avg Reward is ==> 123.57585389489462\n",
            "Episode * 800 * Avg Steps 277.07 * Episodic Reward is ==> -87.09278509588904 * Lastest 100 Episods Avg Reward is ==> 107.91156219577222\n",
            "Episode * 810 * Avg Steps 286.78 * Episodic Reward is ==> 255.3773882773094 * Lastest 100 Episods Avg Reward is ==> 97.52457593996542\n",
            "Episode * 820 * Avg Steps 297.01 * Episodic Reward is ==> -196.78392582949715 * Lastest 100 Episods Avg Reward is ==> 81.43945268680098\n",
            "Episode * 830 * Avg Steps 300.75 * Episodic Reward is ==> 287.39450248682806 * Lastest 100 Episods Avg Reward is ==> 71.9001250177283\n",
            "Episode * 840 * Avg Steps 305.84 * Episodic Reward is ==> -16.217197094843684 * Lastest 100 Episods Avg Reward is ==> 62.99942718654334\n",
            "Episode * 850 * Avg Steps 320.61 * Episodic Reward is ==> 254.75918160883782 * Lastest 100 Episods Avg Reward is ==> 54.535826036205826\n",
            "Episode * 860 * Avg Steps 305.16 * Episodic Reward is ==> -61.682951886230015 * Lastest 100 Episods Avg Reward is ==> 74.32050931452554\n",
            "Episode * 870 * Avg Steps 296.88 * Episodic Reward is ==> 219.77159183082057 * Lastest 100 Episods Avg Reward is ==> 84.84490307100809\n",
            "Episode * 880 * Avg Steps 293.35 * Episodic Reward is ==> -38.12793628518641 * Lastest 100 Episods Avg Reward is ==> 99.03675195886375\n",
            "Episode * 890 * Avg Steps 283.75 * Episodic Reward is ==> -84.55942232129503 * Lastest 100 Episods Avg Reward is ==> 86.3853743229298\n",
            "Episode * 900 * Avg Steps 279.75 * Episodic Reward is ==> 303.3160372611529 * Lastest 100 Episods Avg Reward is ==> 77.24066050655918\n",
            "Episode * 910 * Avg Steps 271.72 * Episodic Reward is ==> 312.2917522526035 * Lastest 100 Episods Avg Reward is ==> 81.15031496365685\n",
            "Episode * 920 * Avg Steps 258.82 * Episodic Reward is ==> -207.6442605046043 * Lastest 100 Episods Avg Reward is ==> 85.29320632265177\n",
            "Episode * 930 * Avg Steps 256.42 * Episodic Reward is ==> 276.05046453059174 * Lastest 100 Episods Avg Reward is ==> 97.38016870663901\n",
            "Episode * 940 * Avg Steps 234.79 * Episodic Reward is ==> 249.38167066646855 * Lastest 100 Episods Avg Reward is ==> 117.34196429719825\n",
            "Episode * 950 * Avg Steps 222.3 * Episodic Reward is ==> 204.58714264314702 * Lastest 100 Episods Avg Reward is ==> 137.36388183484473\n",
            "Episode * 960 * Avg Steps 226.44 * Episodic Reward is ==> 275.0409975739757 * Lastest 100 Episods Avg Reward is ==> 154.43065822044696\n",
            "Episode * 970 * Avg Steps 228.91 * Episodic Reward is ==> 22.2764198062994 * Lastest 100 Episods Avg Reward is ==> 174.59260599745102\n",
            "Episode * 980 * Avg Steps 229.45 * Episodic Reward is ==> 221.10446093747538 * Lastest 100 Episods Avg Reward is ==> 186.69288607367196\n",
            "Episode * 990 * Avg Steps 237.3 * Episodic Reward is ==> 263.939686947621 * Lastest 100 Episods Avg Reward is ==> 209.20116553896983\n",
            "Episode * 1000 * Avg Steps 236.57 * Episodic Reward is ==> 244.8700862551186 * Lastest 100 Episods Avg Reward is ==> 221.80548203995528\n",
            "Episode * 1010 * Avg Steps 235.73 * Episodic Reward is ==> 277.6239833458669 * Lastest 100 Episods Avg Reward is ==> 223.71214225799704\n",
            "Episode * 1020 * Avg Steps 240.18 * Episodic Reward is ==> 293.59899026852395 * Lastest 100 Episods Avg Reward is ==> 234.59722840613634\n",
            "Episode * 1030 * Avg Steps 234.82 * Episodic Reward is ==> 255.59825833503814 * Lastest 100 Episods Avg Reward is ==> 240.28488038616328\n",
            "Episode * 1040 * Avg Steps 230.74 * Episodic Reward is ==> 254.27623143453948 * Lastest 100 Episods Avg Reward is ==> 241.46436008720934\n",
            "Episode * 1050 * Avg Steps 233.9 * Episodic Reward is ==> 247.24025800006794 * Lastest 100 Episods Avg Reward is ==> 244.05821572931276\n",
            "Episode * 1060 * Avg Steps 238.68 * Episodic Reward is ==> 288.9211643327874 * Lastest 100 Episods Avg Reward is ==> 245.84718222917178\n",
            "Episode * 1070 * Avg Steps 242.29 * Episodic Reward is ==> 162.39646156591758 * Lastest 100 Episods Avg Reward is ==> 248.2195987471806\n",
            "Episode * 1080 * Avg Steps 242.46 * Episodic Reward is ==> 312.93891023907634 * Lastest 100 Episods Avg Reward is ==> 251.2652367919402\n",
            "Episode * 1090 * Avg Steps 240.02 * Episodic Reward is ==> 250.5394189101747 * Lastest 100 Episods Avg Reward is ==> 253.62938407524862\n",
            "Episode * 1100 * Avg Steps 229.26 * Episodic Reward is ==> 272.2790764310263 * Lastest 100 Episods Avg Reward is ==> 266.11597384445713\n",
            "Episode * 1110 * Avg Steps 225.59 * Episodic Reward is ==> 301.25600292389026 * Lastest 100 Episods Avg Reward is ==> 272.03717868827516\n",
            "Episode * 1120 * Avg Steps 224.32 * Episodic Reward is ==> 282.0053603631535 * Lastest 100 Episods Avg Reward is ==> 267.75158232446864\n",
            "Episode * 1130 * Avg Steps 225.05 * Episodic Reward is ==> 279.52829185921246 * Lastest 100 Episods Avg Reward is ==> 267.7968786214969\n",
            "Episode * 1140 * Avg Steps 234.45 * Episodic Reward is ==> 262.9204092814641 * Lastest 100 Episods Avg Reward is ==> 268.54561245631584\n",
            "Episode * 1150 * Avg Steps 224.48 * Episodic Reward is ==> 264.61242370115053 * Lastest 100 Episods Avg Reward is ==> 269.6194644904163\n",
            "Episode * 1160 * Avg Steps 235.02 * Episodic Reward is ==> 281.3877843910296 * Lastest 100 Episods Avg Reward is ==> 266.3552155901732\n",
            "Episode * 1170 * Avg Steps 232.02 * Episodic Reward is ==> 243.57996871012116 * Lastest 100 Episods Avg Reward is ==> 266.02528729545844\n",
            "Episode * 1180 * Avg Steps 224.26 * Episodic Reward is ==> 308.05322771048407 * Lastest 100 Episods Avg Reward is ==> 264.16232666548285\n",
            "Episode * 1190 * Avg Steps 220.33 * Episodic Reward is ==> 294.50646236485574 * Lastest 100 Episods Avg Reward is ==> 264.7972555839535\n",
            "Episode * 1200 * Avg Steps 232.32 * Episodic Reward is ==> 273.24846768454455 * Lastest 100 Episods Avg Reward is ==> 262.7425169156133\n",
            "Episode * 1210 * Avg Steps 234.64 * Episodic Reward is ==> 268.2464667301068 * Lastest 100 Episods Avg Reward is ==> 262.489919242488\n",
            "Episode * 1220 * Avg Steps 236.69 * Episodic Reward is ==> 272.83471627352185 * Lastest 100 Episods Avg Reward is ==> 267.1094048372726\n",
            "Episode * 1230 * Avg Steps 245.51 * Episodic Reward is ==> 261.2638982834651 * Lastest 100 Episods Avg Reward is ==> 266.34869097321683\n",
            "Episode * 1240 * Avg Steps 255.59 * Episodic Reward is ==> 272.9438539709347 * Lastest 100 Episods Avg Reward is ==> 263.0488741215629\n",
            "Episode * 1250 * Avg Steps 279.86 * Episodic Reward is ==> 278.6931938644161 * Lastest 100 Episods Avg Reward is ==> 259.5352955694143\n",
            "Episode * 1260 * Avg Steps 271.82 * Episodic Reward is ==> 277.06218229056606 * Lastest 100 Episods Avg Reward is ==> 262.42942007947426\n",
            "Episode * 1270 * Avg Steps 268.0 * Episodic Reward is ==> 293.0611175663592 * Lastest 100 Episods Avg Reward is ==> 264.44770917656825\n",
            "Episode * 1280 * Avg Steps 284.46 * Episodic Reward is ==> 302.57753544752205 * Lastest 100 Episods Avg Reward is ==> 265.1439243940772\n",
            "Episode * 1290 * Avg Steps 303.69 * Episodic Reward is ==> 15.515034707155621 * Lastest 100 Episods Avg Reward is ==> 260.3199723278105\n",
            "Episode * 1300 * Avg Steps 294.66 * Episodic Reward is ==> 260.9346537290506 * Lastest 100 Episods Avg Reward is ==> 261.28554977500795\n",
            "Episode * 1310 * Avg Steps 289.66 * Episodic Reward is ==> 270.52558669224163 * Lastest 100 Episods Avg Reward is ==> 258.33719908548767\n",
            "Episode * 1320 * Avg Steps 290.25 * Episodic Reward is ==> 283.0923915666066 * Lastest 100 Episods Avg Reward is ==> 257.5666839983498\n",
            "Episode * 1330 * Avg Steps 282.2 * Episodic Reward is ==> 259.1220741933407 * Lastest 100 Episods Avg Reward is ==> 254.35457641602\n",
            "Episode * 1340 * Avg Steps 264.53 * Episodic Reward is ==> 262.45345735495647 * Lastest 100 Episods Avg Reward is ==> 255.81647960754213\n",
            "Episode * 1350 * Avg Steps 250.87 * Episodic Reward is ==> 241.65742429289284 * Lastest 100 Episods Avg Reward is ==> 257.50557266039283\n",
            "Episode * 1360 * Avg Steps 255.66 * Episodic Reward is ==> 238.05676469456907 * Lastest 100 Episods Avg Reward is ==> 253.13494483648432\n",
            "Episode * 1370 * Avg Steps 264.8 * Episodic Reward is ==> 260.76434451480156 * Lastest 100 Episods Avg Reward is ==> 242.815193467825\n",
            "Episode * 1380 * Avg Steps 251.36 * Episodic Reward is ==> 23.474700063532538 * Lastest 100 Episods Avg Reward is ==> 238.68637369405707\n",
            "Episode * 1390 * Avg Steps 233.14 * Episodic Reward is ==> 219.49890306800955 * Lastest 100 Episods Avg Reward is ==> 234.408469923898\n",
            "Episode * 1400 * Avg Steps 232.86 * Episodic Reward is ==> 241.040939522228 * Lastest 100 Episods Avg Reward is ==> 233.73602269142918\n",
            "Episode * 1410 * Avg Steps 241.75 * Episodic Reward is ==> 208.4872963349459 * Lastest 100 Episods Avg Reward is ==> 226.43920822963221\n",
            "Episode * 1420 * Avg Steps 245.07 * Episodic Reward is ==> 269.0425344916383 * Lastest 100 Episods Avg Reward is ==> 222.79135272217619\n",
            "Episode * 1430 * Avg Steps 243.99 * Episodic Reward is ==> 255.54046639000734 * Lastest 100 Episods Avg Reward is ==> 225.88955616185476\n",
            "Episode * 1440 * Avg Steps 242.7 * Episodic Reward is ==> 297.8974126742477 * Lastest 100 Episods Avg Reward is ==> 226.126147253308\n",
            "Episode * 1450 * Avg Steps 236.37 * Episodic Reward is ==> 281.5611376623674 * Lastest 100 Episods Avg Reward is ==> 228.23648052132114\n",
            "Episode * 1460 * Avg Steps 234.88 * Episodic Reward is ==> 293.48154622325757 * Lastest 100 Episods Avg Reward is ==> 232.41120379148288\n",
            "Episode * 1470 * Avg Steps 225.97 * Episodic Reward is ==> 271.56700965012647 * Lastest 100 Episods Avg Reward is ==> 242.92213564093592\n",
            "Episode * 1480 * Avg Steps 222.98 * Episodic Reward is ==> 271.1270158029323 * Lastest 100 Episods Avg Reward is ==> 245.04366004896423\n",
            "Episode * 1490 * Avg Steps 224.68 * Episodic Reward is ==> 281.1059593027792 * Lastest 100 Episods Avg Reward is ==> 254.25281227850314\n",
            "Episode * 1500 * Avg Steps 229.76 * Episodic Reward is ==> 236.97082943326933 * Lastest 100 Episods Avg Reward is ==> 252.8410594663445\n",
            "Episode * 1510 * Avg Steps 222.28 * Episodic Reward is ==> 300.38192713889896 * Lastest 100 Episods Avg Reward is ==> 261.9162308615931\n",
            "Episode * 1520 * Avg Steps 209.3 * Episodic Reward is ==> 271.89180533509307 * Lastest 100 Episods Avg Reward is ==> 264.3047718676961\n",
            "Episode * 1530 * Avg Steps 214.42 * Episodic Reward is ==> 291.6618424162782 * Lastest 100 Episods Avg Reward is ==> 265.94183982458094\n",
            "Episode * 1540 * Avg Steps 218.54 * Episodic Reward is ==> 313.03942465615694 * Lastest 100 Episods Avg Reward is ==> 269.8800994482102\n",
            "Episode * 1550 * Avg Steps 216.2 * Episodic Reward is ==> 271.0135732844892 * Lastest 100 Episods Avg Reward is ==> 270.0961594953366\n",
            "Episode * 1560 * Avg Steps 207.22 * Episodic Reward is ==> 273.4674222886903 * Lastest 100 Episods Avg Reward is ==> 264.42694946895006\n",
            "Episode * 1570 * Avg Steps 221.33 * Episodic Reward is ==> 246.96975714948547 * Lastest 100 Episods Avg Reward is ==> 264.8986025126578\n",
            "Episode * 1580 * Avg Steps 233.38 * Episodic Reward is ==> 235.44558175171 * Lastest 100 Episods Avg Reward is ==> 265.71010344815915\n",
            "Episode * 1590 * Avg Steps 229.47 * Episodic Reward is ==> 258.5444733098344 * Lastest 100 Episods Avg Reward is ==> 264.6654569261488\n",
            "Episode * 1600 * Avg Steps 227.98 * Episodic Reward is ==> 269.8680907898166 * Lastest 100 Episods Avg Reward is ==> 267.71340169282934\n",
            "Episode * 1610 * Avg Steps 226.26 * Episodic Reward is ==> 286.5175298686945 * Lastest 100 Episods Avg Reward is ==> 269.4553302778097\n",
            "Episode * 1620 * Avg Steps 234.36 * Episodic Reward is ==> 291.0472593079867 * Lastest 100 Episods Avg Reward is ==> 267.03265578921344\n",
            "Episode * 1630 * Avg Steps 227.55 * Episodic Reward is ==> 268.3396970021886 * Lastest 100 Episods Avg Reward is ==> 266.6244604806268\n",
            "Episode * 1640 * Avg Steps 223.69 * Episodic Reward is ==> 259.19753259443496 * Lastest 100 Episods Avg Reward is ==> 264.2841060690335\n",
            "Episode * 1650 * Avg Steps 226.41 * Episodic Reward is ==> 288.63888170830813 * Lastest 100 Episods Avg Reward is ==> 264.56131846080007\n",
            "Episode * 1660 * Avg Steps 232.65 * Episodic Reward is ==> 240.1971187433894 * Lastest 100 Episods Avg Reward is ==> 265.41665113742994\n",
            "Episode * 1670 * Avg Steps 220.49 * Episodic Reward is ==> 254.8937126859119 * Lastest 100 Episods Avg Reward is ==> 261.20073221386593\n",
            "Episode * 1680 * Avg Steps 210.43 * Episodic Reward is ==> 281.17278286745363 * Lastest 100 Episods Avg Reward is ==> 261.60145910751174\n",
            "Episode * 1690 * Avg Steps 222.13 * Episodic Reward is ==> 238.08107982134442 * Lastest 100 Episods Avg Reward is ==> 258.69625523000434\n",
            "Episode * 1700 * Avg Steps 216.63 * Episodic Reward is ==> 275.4096510029341 * Lastest 100 Episods Avg Reward is ==> 258.41876549850707\n",
            "Episode * 1710 * Avg Steps 219.39 * Episodic Reward is ==> 314.4025405637349 * Lastest 100 Episods Avg Reward is ==> 257.34563673063093\n",
            "Episode * 1720 * Avg Steps 215.25 * Episodic Reward is ==> 266.5956853000575 * Lastest 100 Episods Avg Reward is ==> 259.5767208512116\n",
            "Episode * 1730 * Avg Steps 215.93 * Episodic Reward is ==> 292.63007326346286 * Lastest 100 Episods Avg Reward is ==> 259.542814289378\n",
            "Episode * 1740 * Avg Steps 216.12 * Episodic Reward is ==> 306.1715915055509 * Lastest 100 Episods Avg Reward is ==> 260.11673733637315\n",
            "Episode * 1750 * Avg Steps 209.84 * Episodic Reward is ==> 263.80387216853677 * Lastest 100 Episods Avg Reward is ==> 259.7308960427968\n",
            "Episode * 1760 * Avg Steps 201.55 * Episodic Reward is ==> 261.27407150577017 * Lastest 100 Episods Avg Reward is ==> 263.8213927918672\n",
            "Episode * 1770 * Avg Steps 200.3 * Episodic Reward is ==> 263.9402382584717 * Lastest 100 Episods Avg Reward is ==> 267.3732248635149\n",
            "Episode * 1780 * Avg Steps 198.04 * Episodic Reward is ==> 253.12724695561576 * Lastest 100 Episods Avg Reward is ==> 268.08174460517944\n",
            "Episode * 1790 * Avg Steps 186.16 * Episodic Reward is ==> 316.34999095897984 * Lastest 100 Episods Avg Reward is ==> 268.7388064807643\n",
            "Episode * 1800 * Avg Steps 186.02 * Episodic Reward is ==> 288.8165481650006 * Lastest 100 Episods Avg Reward is ==> 264.16419507775294\n",
            "Episode * 1810 * Avg Steps 188.59 * Episodic Reward is ==> 284.484414870588 * Lastest 100 Episods Avg Reward is ==> 261.3398710638562\n",
            "Episode * 1820 * Avg Steps 186.31 * Episodic Reward is ==> 288.66871730603117 * Lastest 100 Episods Avg Reward is ==> 264.480031415354\n",
            "Episode * 1830 * Avg Steps 189.88 * Episodic Reward is ==> 284.4511212214477 * Lastest 100 Episods Avg Reward is ==> 265.07105601834587\n",
            "Episode * 1840 * Avg Steps 197.13 * Episodic Reward is ==> 312.1707382478876 * Lastest 100 Episods Avg Reward is ==> 265.31366513415566\n",
            "Episode * 1850 * Avg Steps 202.81 * Episodic Reward is ==> 276.93706566286534 * Lastest 100 Episods Avg Reward is ==> 265.13841849413745\n",
            "Episode * 1860 * Avg Steps 206.94 * Episodic Reward is ==> 306.4279243810016 * Lastest 100 Episods Avg Reward is ==> 265.12622334700217\n",
            "Episode * 1870 * Avg Steps 216.11 * Episodic Reward is ==> 273.2846940435073 * Lastest 100 Episods Avg Reward is ==> 263.6356488891917\n",
            "Episode * 1880 * Avg Steps 226.11 * Episodic Reward is ==> 300.88341197717295 * Lastest 100 Episods Avg Reward is ==> 262.5194081590363\n",
            "Episode * 1890 * Avg Steps 230.99 * Episodic Reward is ==> 290.9472612194803 * Lastest 100 Episods Avg Reward is ==> 264.80858006311576\n",
            "Episode * 1900 * Avg Steps 237.11 * Episodic Reward is ==> 285.5138771786843 * Lastest 100 Episods Avg Reward is ==> 269.47581485398996\n",
            "Episode * 1910 * Avg Steps 234.45 * Episodic Reward is ==> 285.48400867341695 * Lastest 100 Episods Avg Reward is ==> 272.90131068083195\n",
            "Episode * 1920 * Avg Steps 240.79 * Episodic Reward is ==> 279.6954924978044 * Lastest 100 Episods Avg Reward is ==> 273.0456199938192\n",
            "Episode * 1930 * Avg Steps 237.35 * Episodic Reward is ==> 288.34650959232033 * Lastest 100 Episods Avg Reward is ==> 272.1050371290063\n",
            "Episode * 1940 * Avg Steps 227.45 * Episodic Reward is ==> 268.07685162906614 * Lastest 100 Episods Avg Reward is ==> 271.79361738923586\n",
            "Episode * 1950 * Avg Steps 225.19 * Episodic Reward is ==> 293.42501479736853 * Lastest 100 Episods Avg Reward is ==> 272.76292630914105\n",
            "Episode * 1960 * Avg Steps 228.45 * Episodic Reward is ==> 264.0636194268345 * Lastest 100 Episods Avg Reward is ==> 274.93830669768187\n",
            "Episode * 1970 * Avg Steps 216.12 * Episodic Reward is ==> 301.0341575231598 * Lastest 100 Episods Avg Reward is ==> 273.1932238820246\n",
            "Episode * 1980 * Avg Steps 212.18 * Episodic Reward is ==> 284.2877133720988 * Lastest 100 Episods Avg Reward is ==> 273.8015649716942\n",
            "Episode * 1990 * Avg Steps 223.57 * Episodic Reward is ==> 246.3215694654514 * Lastest 100 Episods Avg Reward is ==> 266.7327549387798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_98108f60-87ac-49d5-9235-c586b0fa9ce4\", \"ddpg_per_True_20210331_175855.csv\", 91639)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuF4rIYqwyZ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "82d3254f-38f4-4036-d025-b0ec836998ff"
      },
      "source": [
        "ddpg.actor_model.save(\"new_ddpg_baseversion_withoutnoise_per_actor_final_ep.h5\")\n",
        "ddpg.critic_model.save(\"new_ddpg_baseversion_withoutnoise_per_critic_final_ep.h5\")\n",
        "files.download(\"new_ddpg_baseversion_withoutnoise_per_actor_final_ep.h5\")\n",
        "files.download(\"new_ddpg_baseversion_withoutnoise_per_critic_final_ep.h5\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2c6ea1ca-8946-4d87-823f-45389a857d93\", \"new_ddpg_baseversion_withoutnoise_per_actor_final_ep.h5\", 563712)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_019a47a2-411d-44d2-81e3-2494c08eee94\", \"new_ddpg_baseversion_withoutnoise_per_critic_final_ep.h5\", 767808)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYhtvRUQY_Ov"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}