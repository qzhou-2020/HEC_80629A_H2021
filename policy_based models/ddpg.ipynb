{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Otdpa1QZV19H",
    "outputId": "2898249b-870f-474c-f1fb-03a88fa491db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting box2d-py\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 7.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.8\n",
      "Collecting gym[Box_2D]\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f2/e7ee20bf02b2d02263becba1c5ec4203fef7cfbd57759e040e51307173f4/gym-0.18.0.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 7.2MB/s \n",
      "\u001b[33m  WARNING: gym 0.18.0 does not provide the extra 'box_2d'\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/91/ee427c42957f8c4cbe477bf4f8b7f608e003a17941e509d1777e58648cb3/scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
      "\u001b[K     |████████████████████████████████| 27.4MB 110kB/s \n",
      "\u001b[?25hCollecting numpy>=1.10.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3MB 119kB/s \n",
      "\u001b[?25hCollecting pyglet<=1.5.0,>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 45.3MB/s \n",
      "\u001b[?25hCollecting Pillow<=7.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/f2/6722dd0c22e3a143ac792ccb2424924ac72af4adea756b1165b4cad50da7/Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 47.4MB/s \n",
      "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Collecting future\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 48.5MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.18.0-cp37-none-any.whl size=1656450 sha256=84722b3b257992eba32ba51028be38a810a23f5f4ab2a920deb15b127fa62515\n",
      "  Stored in directory: /root/.cache/pip/wheels/be/85/3b/480b828a4a697b37392740a040b8989f729d952b4e441a1877\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=bfd388ca9ed893821fa1b99e2a6c86cab08cf099f359fcf432365cbb5687aed4\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built gym future\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, scipy, future, pyglet, Pillow, cloudpickle, gym\n",
      "  Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: pyglet 1.5.0\n",
      "    Uninstalling pyglet-1.5.0:\n",
      "      Successfully uninstalled pyglet-1.5.0\n",
      "  Found existing installation: Pillow 7.1.2\n",
      "    Uninstalling Pillow-7.1.2:\n",
      "      Successfully uninstalled Pillow-7.1.2\n",
      "  Found existing installation: cloudpickle 1.3.0\n",
      "    Uninstalling cloudpickle-1.3.0:\n",
      "      Successfully uninstalled cloudpickle-1.3.0\n",
      "  Found existing installation: gym 0.17.3\n",
      "    Uninstalling gym-0.17.3:\n",
      "      Successfully uninstalled gym-0.17.3\n",
      "Successfully installed Pillow-7.2.0 cloudpickle-1.6.0 future-0.18.2 gym-0.18.0 numpy-1.20.2 pyglet-1.5.0 scipy-1.6.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL",
         "numpy"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall box2d-py\n",
    "!pip install --upgrade --force-reinstall gym[Box_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpaMSP_LwgQp"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNXpnq0jt4uO"
   },
   "outputs": [],
   "source": [
    "# This implementation is modified from Keras examples\n",
    "# https://keras.io/examples/rl/ddpg_pendulum/\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,activations\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RANDOM_SEEDS = 123\n",
    "\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZi8mrsrt9HJ"
   },
   "outputs": [],
   "source": [
    "# Ornstein-Uhlenbeck process\n",
    "class OUNoise:\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (self.x_prev \n",
    "            + self.theta * (self.mu - self.x_prev) * self.dt \n",
    "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape))\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x_initial if self.x_initial is not None else np.zeros_like(self.mu)\n",
    "\n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is a modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "    Story data with its priority in the tree.\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx]:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n",
    "\n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This Memory class is modified based on the original code from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.sample_count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sample_count\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)   # set the max p for new p\n",
    "        self.sample_count = min(self.sample_count + 1, self.tree.capacity)\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
    "        pri_seg = self.tree.total_p / n       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        a = self.tree.tree[-self.tree.capacity:]\n",
    "        min_prob = np.min(a[a != 0]) / self.tree.total_p     # for later calculate ISweight\n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.total_p\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i, :] = idx, data\n",
    "\n",
    "        return b_idx, b_memory, ISWeights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "\n",
    "\n",
    "def get_actor(state_shape, action_dim, upper_bound, units=(512,256)):\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.004, maxval=0.004)\n",
    "    inputs = layers.Input(shape=(state_shape,))\n",
    "    for idx, unit in enumerate(units):\n",
    "        if idx == 0:\n",
    "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(inputs)\n",
    "        else:\n",
    "            out = layers.Dense(unit, name=f\"lvl{idx+1}\",activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(action_dim, name=\"Output\", activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    scaled_outputs = outputs * upper_bound\n",
    "    model = Model(inputs,scaled_outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic(state_shape, action_dim, state_units=(512,256),action_units=128,concat_units=128):\n",
    "    state_input = layers.Input(shape=(state_shape), name=\"state_input\")\n",
    "    for idx, unit in enumerate(state_units):      \n",
    "        if idx == 0:\n",
    "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_input)\n",
    "        else:\n",
    "            state_out = layers.Dense(unit, name=f\"state_lvl{idx+1}\", activation=\"relu\")(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(action_dim),name=\"action_input\")\n",
    "    action_out = layers.Dense(action_units, name=\"action_lvl\", activation=\"relu\")(action_input)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out,action_out])\n",
    "    out = layers.Dense(concat_units, name=\"concat_lvl\",activation=\"relu\")(concat)\n",
    "    outputs = layers.Dense(1)(out) # Q value\n",
    "\n",
    "    model = Model([state_input, action_input], outputs)\n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def update_target(model, target_model, tau=0.001):\n",
    "    weights = model.variables\n",
    "    target_weights = target_model.variables\n",
    "    for (a,b) in zip(target_weights,weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP_5PWKuhj6T"
   },
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 per=False,\n",
    "                 actor_lr=0.00005,\n",
    "                 critic_lr=0.0005,\n",
    "                 actor_units=(512,256),\n",
    "                 state_units=(512,256),\n",
    "                 action_units=128,\n",
    "                 concat_units=128,\n",
    "                 noise=\"OU\",\n",
    "                 tau=0.001,\n",
    "                 gamma=0.99,\n",
    "                 batch_size=64,\n",
    "                 memory_size=2**17\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.per = per\n",
    "        self.state_shape = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.upper_bound = env.action_space.high[0]\n",
    "        self.lower_bound = env.action_space.low[0]\n",
    "        if noise == \"OU\":\n",
    "            self.noise = OUNoise(mu=np.zeros(self.action_dim),sigma=float(0.2)*np.ones(self.action_dim))\n",
    "        if noise == \"gaussian\":\n",
    "            self.noise = np.random.normal(scale=0.2, size=self.action_dim)\n",
    "        if noise == \"None\":\n",
    "            self.noise = 0\n",
    "        self.actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
    "        self.critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
    "        self.target_actor_model = get_actor(self.state_shape, self.action_dim, self.upper_bound, actor_units)\n",
    "        self.target_critic_model = get_critic(self.state_shape, self.action_dim, state_units, action_units, concat_units)\n",
    "        self.target_actor_model.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic_model.set_weights(self.critic_model.get_weights())\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = Memory(capacity=memory_size) if per else deque(maxlen=memory_size)\n",
    "        self.alpha_decay = 0.999\n",
    "\n",
    "    def policy(self, state, noise_object):\n",
    "        sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "        if self.noise == \"OU\":\n",
    "            noise = noise_object()\n",
    "        else:\n",
    "            noise = noise_object\n",
    "        sampled_actions = sampled_actions.numpy() + noise\n",
    "        legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
    "        return np.squeeze(legal_action)\n",
    "\n",
    "    def record(self, obs_array):\n",
    "        if self.per:\n",
    "            transition = np.hstack(obs_array)\n",
    "            self.memory.store(transition)\n",
    "        else:\n",
    "            self.memory.append(obs_array)    \n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def per_update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic_model(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            ) * (1. - done_batch)\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            errors = y - critic_value\n",
    "            critic_loss = tf.math.reduce_mean(ISWeights * tf.math.square(errors))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        abs_errors = tf.reduce_sum(tf.abs(errors), axis=1)\n",
    "        #self.memory.batch_update(tree_idx, abs_errors)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "        return abs_errors\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor_model(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic_model(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            ) * (1. - done_batch)\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.keras.losses.Huber()(y,critic_value)\n",
    "\n",
    "            critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "            self.critic_optimizer.apply_gradients(\n",
    "                zip(critic_grad, self.critic_model.trainable_variables)\n",
    "            )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        if self.per:\n",
    "            tree_idx, samples, ISWeights = self.memory.sample(min(self.batch_size,len(self.memory)))\n",
    "            ISWeights_batch = tf.convert_to_tensor(ISWeights,dtype=tf.float32)\n",
    "            split_shape = np.cumsum([self.state_shape, self.action_dim, 1, self.state_shape])\n",
    "            states, actions, rewards, next_states, dones = np.hsplit(samples, split_shape)\n",
    "            state_batch = tf.convert_to_tensor(states)\n",
    "            action_batch = tf.convert_to_tensor(actions)\n",
    "            reward_batch = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_state_batch = tf.convert_to_tensor(next_states)\n",
    "            done_batch = tf.convert_to_tensor(dones,dtype=tf.float32)\n",
    "            abs_errors = self.per_update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, ISWeights_batch)\n",
    "            self.memory.batch_update(tree_idx, abs_errors.numpy())\n",
    "        else:\n",
    "            samples = random.sample(self.memory, min(self.batch_size,len(self.memory)))\n",
    "            trans_s = np.array(samples,dtype=object).T\n",
    "            state_batch = tf.convert_to_tensor(np.row_stack(trans_s[0]))\n",
    "            action_batch = tf.convert_to_tensor(np.row_stack(trans_s[1]))\n",
    "            reward_batch = tf.convert_to_tensor(np.row_stack(trans_s[2]),dtype=tf.float32)\n",
    "            next_state_batch = tf.convert_to_tensor(np.row_stack(trans_s[3]))\n",
    "            done_batch = tf.convert_to_tensor(np.row_stack(trans_s[4]),dtype=tf.float32)\n",
    "            self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "\n",
    "    def train(self, max_episodes=2000, max_steps=1000):\n",
    "        self.ep_reward_list = []\n",
    "        self.avg_reward_list = []\n",
    "        self.ep_steps_list = []\n",
    "\n",
    "        for ep in range(max_episodes):\n",
    "            prev_state = self.env.reset()\n",
    "            episodic_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < max_steps:\n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "                action = self.policy(tf_prev_state, self.noise)\n",
    "\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                self.record([prev_state, action, reward, state, done])\n",
    "\n",
    "                episodic_reward += reward\n",
    "                self.replay()\n",
    "\n",
    "                update_target(self.actor_model, self.target_actor_model, self.tau)\n",
    "                update_target(self.critic_model, self.target_critic_model, self.tau)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "                steps += 1\n",
    "            #if self.per == True:\n",
    "            #    self.memory.alpha *= self.alpha_decay\n",
    "            self.ep_reward_list.append(episodic_reward)\n",
    "            # Mean of last 100 episodes\n",
    "            avg_reward = np.mean(self.ep_reward_list[-100:])\n",
    "            self.avg_reward_list.append(avg_reward)\n",
    "            self.ep_steps_list.append(steps)\n",
    "            avg_step = np.mean(self.ep_steps_list[-100:])\n",
    "            if ep%10 == 0:\n",
    "                print(f\"Episode * {ep} * Avg Steps {avg_step} * Episodic Reward is ==> {episodic_reward} * Lastest 100 Episods Avg Reward is ==> {avg_reward}\")\n",
    "\n",
    "\n",
    "        plt.plot(self.avg_reward_list)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "        plt.show()\n",
    "\n",
    "        content = {\"ep_reward\":self.ep_reward_list,\"avg_reward\":self.avg_reward_list, \"steps\":self.ep_steps_list}\n",
    "        df = pd.DataFrame(content)\n",
    "        df.to_csv(f'ddpg_per_{self.per}_{date_time}.csv') \n",
    "        files.download(f'ddpg_per_{self.per}_{date_time}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOmmC-HiYh16",
    "outputId": "7e5b1728-b6c1-4d02-ba58-bcd3ab563c43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = 'LunarLanderContinuous-v2'\n",
    "gym_env = gym.make(problem)\n",
    "gym_env.seed(RANDOM_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLkJwfK0hvvw"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "ddpg = DDPG(gym_env,noise=\"None\",per=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fb2tBUrvjynI",
    "outputId": "5dc972e5-0751-4603-cb35-177dcce43044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Steps 71.0 * Episodic Reward is ==> -165.80606903415105 * Lastest 100 Episods Avg Reward is ==> -165.80606903415105\n",
      "Episode * 10 * Avg Steps 108.9090909090909 * Episodic Reward is ==> -296.5725269126531 * Lastest 100 Episods Avg Reward is ==> -197.2747373606021\n",
      "Episode * 20 * Avg Steps 114.23809523809524 * Episodic Reward is ==> -262.7804226598744 * Lastest 100 Episods Avg Reward is ==> -209.14528529739488\n",
      "Episode * 30 * Avg Steps 112.64516129032258 * Episodic Reward is ==> -864.4460556614549 * Lastest 100 Episods Avg Reward is ==> -279.7397301456407\n",
      "Episode * 40 * Avg Steps 120.2439024390244 * Episodic Reward is ==> -209.77388898410987 * Lastest 100 Episods Avg Reward is ==> -253.7009882616932\n",
      "Episode * 50 * Avg Steps 127.72549019607843 * Episodic Reward is ==> -196.6534320889486 * Lastest 100 Episods Avg Reward is ==> -242.85430956490157\n",
      "Episode * 60 * Avg Steps 130.0983606557377 * Episodic Reward is ==> 22.34450764794631 * Lastest 100 Episods Avg Reward is ==> -234.73761049206388\n",
      "Episode * 70 * Avg Steps 128.26760563380282 * Episodic Reward is ==> -189.21965261352545 * Lastest 100 Episods Avg Reward is ==> -227.57827230113588\n",
      "Episode * 80 * Avg Steps 126.22222222222223 * Episodic Reward is ==> -187.84212823480453 * Lastest 100 Episods Avg Reward is ==> -227.95334436034713\n",
      "Episode * 90 * Avg Steps 129.15384615384616 * Episodic Reward is ==> -199.72015209241528 * Lastest 100 Episods Avg Reward is ==> -222.5958610713718\n",
      "Episode * 100 * Avg Steps 131.57 * Episodic Reward is ==> -233.08171331873356 * Lastest 100 Episods Avg Reward is ==> -220.94355992904772\n",
      "Episode * 110 * Avg Steps 138.13 * Episodic Reward is ==> -260.8728164959539 * Lastest 100 Episods Avg Reward is ==> -217.9210539050815\n",
      "Episode * 120 * Avg Steps 139.96 * Episodic Reward is ==> -247.4371266611669 * Lastest 100 Episods Avg Reward is ==> -218.7178183500081\n",
      "Episode * 130 * Avg Steps 143.27 * Episodic Reward is ==> -332.2073522248953 * Lastest 100 Episods Avg Reward is ==> -189.6580680485777\n",
      "Episode * 140 * Avg Steps 145.5 * Episodic Reward is ==> -135.48554716263916 * Lastest 100 Episods Avg Reward is ==> -186.88085252244102\n",
      "Episode * 150 * Avg Steps 144.43 * Episodic Reward is ==> -145.01751772696716 * Lastest 100 Episods Avg Reward is ==> -186.6088176326146\n",
      "Episode * 160 * Avg Steps 143.63 * Episodic Reward is ==> -221.38504045139513 * Lastest 100 Episods Avg Reward is ==> -188.29061454404692\n",
      "Episode * 170 * Avg Steps 143.73 * Episodic Reward is ==> -46.08218675686953 * Lastest 100 Episods Avg Reward is ==> -187.96030661386806\n",
      "Episode * 180 * Avg Steps 145.0 * Episodic Reward is ==> -291.7886241191519 * Lastest 100 Episods Avg Reward is ==> -182.77983837824712\n",
      "Episode * 190 * Avg Steps 146.69 * Episodic Reward is ==> -64.41826812159358 * Lastest 100 Episods Avg Reward is ==> -179.6005101620203\n",
      "Episode * 200 * Avg Steps 145.53 * Episodic Reward is ==> -232.23568388487544 * Lastest 100 Episods Avg Reward is ==> -179.00271435010282\n",
      "Episode * 210 * Avg Steps 140.97 * Episodic Reward is ==> -94.07103372720209 * Lastest 100 Episods Avg Reward is ==> -182.34969200083654\n",
      "Episode * 220 * Avg Steps 140.19 * Episodic Reward is ==> -233.06179145482614 * Lastest 100 Episods Avg Reward is ==> -182.47870271406217\n",
      "Episode * 230 * Avg Steps 138.7 * Episodic Reward is ==> -124.08050412861516 * Lastest 100 Episods Avg Reward is ==> -188.10462180996146\n",
      "Episode * 240 * Avg Steps 135.48 * Episodic Reward is ==> -117.0417386838307 * Lastest 100 Episods Avg Reward is ==> -195.67228866111589\n",
      "Episode * 250 * Avg Steps 135.46 * Episodic Reward is ==> -99.97133464449206 * Lastest 100 Episods Avg Reward is ==> -190.41670035267697\n",
      "Episode * 260 * Avg Steps 138.5 * Episodic Reward is ==> -169.9861386180177 * Lastest 100 Episods Avg Reward is ==> -183.6577332781444\n",
      "Episode * 270 * Avg Steps 140.04 * Episodic Reward is ==> -63.7921903912254 * Lastest 100 Episods Avg Reward is ==> -178.09474418662384\n",
      "Episode * 280 * Avg Steps 143.84 * Episodic Reward is ==> -282.50518253215284 * Lastest 100 Episods Avg Reward is ==> -174.52348614197695\n",
      "Episode * 290 * Avg Steps 142.51 * Episodic Reward is ==> 168.67644945824247 * Lastest 100 Episods Avg Reward is ==> -173.95517258271718\n",
      "Episode * 300 * Avg Steps 145.04 * Episodic Reward is ==> -162.62592498285122 * Lastest 100 Episods Avg Reward is ==> -167.9014007639054\n",
      "Episode * 310 * Avg Steps 149.51 * Episodic Reward is ==> -256.89185269163005 * Lastest 100 Episods Avg Reward is ==> -166.38622460311726\n",
      "Episode * 320 * Avg Steps 152.45 * Episodic Reward is ==> -246.67789436666877 * Lastest 100 Episods Avg Reward is ==> -157.58749565832468\n",
      "Episode * 330 * Avg Steps 159.98 * Episodic Reward is ==> -27.310755230196314 * Lastest 100 Episods Avg Reward is ==> -149.46620507809152\n",
      "Episode * 340 * Avg Steps 161.22 * Episodic Reward is ==> -209.00020852972435 * Lastest 100 Episods Avg Reward is ==> -142.55016177098142\n",
      "Episode * 350 * Avg Steps 159.42 * Episodic Reward is ==> -164.57057466161 * Lastest 100 Episods Avg Reward is ==> -139.65865348909557\n",
      "Episode * 360 * Avg Steps 158.83 * Episodic Reward is ==> -243.82033111227082 * Lastest 100 Episods Avg Reward is ==> -137.4280561029909\n",
      "Episode * 370 * Avg Steps 162.09 * Episodic Reward is ==> -78.25418422935587 * Lastest 100 Episods Avg Reward is ==> -138.91385626582883\n",
      "Episode * 380 * Avg Steps 165.27 * Episodic Reward is ==> -166.89202886532735 * Lastest 100 Episods Avg Reward is ==> -136.94541954800485\n",
      "Episode * 390 * Avg Steps 164.84 * Episodic Reward is ==> -93.8699590005653 * Lastest 100 Episods Avg Reward is ==> -132.11083241619485\n",
      "Episode * 400 * Avg Steps 159.78 * Episodic Reward is ==> -196.79830689184854 * Lastest 100 Episods Avg Reward is ==> -135.92424079583182\n",
      "Episode * 410 * Avg Steps 159.37 * Episodic Reward is ==> -96.2550734089816 * Lastest 100 Episods Avg Reward is ==> -128.26610972614492\n",
      "Episode * 420 * Avg Steps 159.85 * Episodic Reward is ==> -243.54859122949884 * Lastest 100 Episods Avg Reward is ==> -130.07821992062455\n",
      "Episode * 430 * Avg Steps 157.02 * Episodic Reward is ==> -73.82004523422827 * Lastest 100 Episods Avg Reward is ==> -136.82853637420624\n",
      "Episode * 440 * Avg Steps 160.41 * Episodic Reward is ==> 26.091756060410386 * Lastest 100 Episods Avg Reward is ==> -131.9524782891625\n",
      "Episode * 450 * Avg Steps 167.7 * Episodic Reward is ==> -149.60984372401578 * Lastest 100 Episods Avg Reward is ==> -133.974686869206\n",
      "Episode * 460 * Avg Steps 166.41 * Episodic Reward is ==> 17.835954956536593 * Lastest 100 Episods Avg Reward is ==> -137.60274736565532\n",
      "Episode * 470 * Avg Steps 167.75 * Episodic Reward is ==> -130.76380464193363 * Lastest 100 Episods Avg Reward is ==> -142.8285461041571\n",
      "Episode * 480 * Avg Steps 165.23 * Episodic Reward is ==> -140.97986109308928 * Lastest 100 Episods Avg Reward is ==> -146.99144013112243\n",
      "Episode * 490 * Avg Steps 166.54 * Episodic Reward is ==> -180.33813577433557 * Lastest 100 Episods Avg Reward is ==> -158.49521674905\n",
      "Episode * 500 * Avg Steps 171.05 * Episodic Reward is ==> -236.7599394434379 * Lastest 100 Episods Avg Reward is ==> -158.88803329563405\n",
      "Episode * 510 * Avg Steps 170.33 * Episodic Reward is ==> -151.0924464875928 * Lastest 100 Episods Avg Reward is ==> -159.21941408851725\n",
      "Episode * 520 * Avg Steps 171.07 * Episodic Reward is ==> -179.6771479326062 * Lastest 100 Episods Avg Reward is ==> -154.9166646185022\n",
      "Episode * 530 * Avg Steps 171.27 * Episodic Reward is ==> -218.08549989442045 * Lastest 100 Episods Avg Reward is ==> -149.3350082939281\n",
      "Episode * 540 * Avg Steps 169.73 * Episodic Reward is ==> -149.17247476881707 * Lastest 100 Episods Avg Reward is ==> -151.8506424269592\n",
      "Episode * 550 * Avg Steps 165.28 * Episodic Reward is ==> -195.7175072936924 * Lastest 100 Episods Avg Reward is ==> -151.60534100299458\n",
      "Episode * 560 * Avg Steps 166.86 * Episodic Reward is ==> -152.63787469841938 * Lastest 100 Episods Avg Reward is ==> -146.81842443893757\n",
      "Episode * 570 * Avg Steps 165.38 * Episodic Reward is ==> -89.18985119776173 * Lastest 100 Episods Avg Reward is ==> -136.77597997890328\n",
      "Episode * 580 * Avg Steps 164.24 * Episodic Reward is ==> -128.73043490828542 * Lastest 100 Episods Avg Reward is ==> -133.25675002687245\n",
      "Episode * 590 * Avg Steps 163.47 * Episodic Reward is ==> -149.30936472934056 * Lastest 100 Episods Avg Reward is ==> -120.0823977598204\n",
      "Episode * 600 * Avg Steps 164.5 * Episodic Reward is ==> -137.0910787889162 * Lastest 100 Episods Avg Reward is ==> -114.85716405072915\n",
      "Episode * 610 * Avg Steps 167.08 * Episodic Reward is ==> -163.83336359062525 * Lastest 100 Episods Avg Reward is ==> -112.15078994821795\n",
      "Episode * 620 * Avg Steps 167.98 * Episodic Reward is ==> -186.4923742452861 * Lastest 100 Episods Avg Reward is ==> -109.91058721804866\n",
      "Episode * 630 * Avg Steps 168.71 * Episodic Reward is ==> -117.31974913064262 * Lastest 100 Episods Avg Reward is ==> -112.50201247409777\n",
      "Episode * 640 * Avg Steps 167.32 * Episodic Reward is ==> -164.16921244095582 * Lastest 100 Episods Avg Reward is ==> -115.53211575683457\n",
      "Episode * 650 * Avg Steps 164.71 * Episodic Reward is ==> -177.93952687716828 * Lastest 100 Episods Avg Reward is ==> -114.84556859949302\n",
      "Episode * 660 * Avg Steps 164.33 * Episodic Reward is ==> -125.70096201413669 * Lastest 100 Episods Avg Reward is ==> -119.59310897641686\n",
      "Episode * 670 * Avg Steps 166.8 * Episodic Reward is ==> -178.6453673666564 * Lastest 100 Episods Avg Reward is ==> -118.2985307980224\n",
      "Episode * 680 * Avg Steps 169.77 * Episodic Reward is ==> -253.22854276284082 * Lastest 100 Episods Avg Reward is ==> -118.75822014670152\n",
      "Episode * 690 * Avg Steps 169.88 * Episodic Reward is ==> -206.78359326784027 * Lastest 100 Episods Avg Reward is ==> -128.26843132643583\n",
      "Episode * 700 * Avg Steps 170.69 * Episodic Reward is ==> -188.76088602619637 * Lastest 100 Episods Avg Reward is ==> -133.64885729737153\n",
      "Episode * 710 * Avg Steps 167.4 * Episodic Reward is ==> -174.60450165447577 * Lastest 100 Episods Avg Reward is ==> -138.74443053298876\n",
      "Episode * 720 * Avg Steps 167.76 * Episodic Reward is ==> -258.7116464328914 * Lastest 100 Episods Avg Reward is ==> -142.27941252439373\n",
      "Episode * 730 * Avg Steps 173.56 * Episodic Reward is ==> 27.21317038542554 * Lastest 100 Episods Avg Reward is ==> -131.19372247455388\n",
      "Episode * 740 * Avg Steps 177.04 * Episodic Reward is ==> -397.94013240106716 * Lastest 100 Episods Avg Reward is ==> -138.05691333480124\n",
      "Episode * 750 * Avg Steps 180.36 * Episodic Reward is ==> -244.16770415988267 * Lastest 100 Episods Avg Reward is ==> -150.2804467269923\n",
      "Episode * 760 * Avg Steps 181.51 * Episodic Reward is ==> -257.19108710683173 * Lastest 100 Episods Avg Reward is ==> -156.3227541318206\n",
      "Episode * 770 * Avg Steps 182.05 * Episodic Reward is ==> -38.826190130011746 * Lastest 100 Episods Avg Reward is ==> -167.39179835956386\n",
      "Episode * 780 * Avg Steps 193.32 * Episodic Reward is ==> -14.897538314813513 * Lastest 100 Episods Avg Reward is ==> -156.44201169637557\n",
      "Episode * 790 * Avg Steps 206.06 * Episodic Reward is ==> -47.43415005832094 * Lastest 100 Episods Avg Reward is ==> -139.2328277323585\n",
      "Episode * 800 * Avg Steps 216.68 * Episodic Reward is ==> 1.6646926898589671 * Lastest 100 Episods Avg Reward is ==> -128.90564413774158\n",
      "Episode * 810 * Avg Steps 229.7 * Episodic Reward is ==> 25.139258405344847 * Lastest 100 Episods Avg Reward is ==> -112.36797845002599\n",
      "Episode * 820 * Avg Steps 234.77 * Episodic Reward is ==> 95.88193863078754 * Lastest 100 Episods Avg Reward is ==> -98.24876096919064\n",
      "Episode * 830 * Avg Steps 235.45 * Episodic Reward is ==> 124.90097428401006 * Lastest 100 Episods Avg Reward is ==> -90.88877335483427\n",
      "Episode * 840 * Avg Steps 241.81 * Episodic Reward is ==> -8.248245750580125 * Lastest 100 Episods Avg Reward is ==> -65.34480235828057\n",
      "Episode * 850 * Avg Steps 248.03 * Episodic Reward is ==> -113.01674408922972 * Lastest 100 Episods Avg Reward is ==> -38.11053951177793\n",
      "Episode * 860 * Avg Steps 249.16 * Episodic Reward is ==> 145.8905901106263 * Lastest 100 Episods Avg Reward is ==> -15.508352131206088\n",
      "Episode * 870 * Avg Steps 253.5 * Episodic Reward is ==> -34.00405940816279 * Lastest 100 Episods Avg Reward is ==> 7.102837458633797\n",
      "Episode * 880 * Avg Steps 248.45 * Episodic Reward is ==> -54.25813132014509 * Lastest 100 Episods Avg Reward is ==> 13.007897583693582\n",
      "Episode * 890 * Avg Steps 246.56 * Episodic Reward is ==> 24.723717354026235 * Lastest 100 Episods Avg Reward is ==> 12.15126898940493\n",
      "Episode * 900 * Avg Steps 248.53 * Episodic Reward is ==> 32.64809153793501 * Lastest 100 Episods Avg Reward is ==> 24.838738041185906\n",
      "Episode * 910 * Avg Steps 249.61 * Episodic Reward is ==> 91.29190933166629 * Lastest 100 Episods Avg Reward is ==> 30.296477393775216\n",
      "Episode * 920 * Avg Steps 256.06 * Episodic Reward is ==> 114.98565030244006 * Lastest 100 Episods Avg Reward is ==> 38.19709067151599\n",
      "Episode * 930 * Avg Steps 261.2 * Episodic Reward is ==> 64.4657913910929 * Lastest 100 Episods Avg Reward is ==> 44.49714014802832\n",
      "Episode * 940 * Avg Steps 266.27 * Episodic Reward is ==> 86.49703752084815 * Lastest 100 Episods Avg Reward is ==> 48.7938888562228\n",
      "Episode * 950 * Avg Steps 273.55 * Episodic Reward is ==> -15.085397516891723 * Lastest 100 Episods Avg Reward is ==> 49.23369008503848\n",
      "Episode * 960 * Avg Steps 285.49 * Episodic Reward is ==> 83.24526896795562 * Lastest 100 Episods Avg Reward is ==> 53.82338302230412\n",
      "Episode * 970 * Avg Steps 291.67 * Episodic Reward is ==> 89.36852630914466 * Lastest 100 Episods Avg Reward is ==> 58.12950058886206\n",
      "Episode * 980 * Avg Steps 296.72 * Episodic Reward is ==> -22.174834721775337 * Lastest 100 Episods Avg Reward is ==> 55.9727091987511\n",
      "Episode * 990 * Avg Steps 297.75 * Episodic Reward is ==> -31.861380533581727 * Lastest 100 Episods Avg Reward is ==> 47.90288528490878\n",
      "Episode * 1000 * Avg Steps 291.91 * Episodic Reward is ==> -145.56110261851273 * Lastest 100 Episods Avg Reward is ==> 34.57442444881713\n",
      "Episode * 1010 * Avg Steps 282.3 * Episodic Reward is ==> -133.82563849760209 * Lastest 100 Episods Avg Reward is ==> 14.923162673131879\n",
      "Episode * 1020 * Avg Steps 276.93 * Episodic Reward is ==> -105.42572982164805 * Lastest 100 Episods Avg Reward is ==> -3.982348868686389\n",
      "Episode * 1030 * Avg Steps 271.24 * Episodic Reward is ==> -76.57893773612575 * Lastest 100 Episods Avg Reward is ==> -20.251730982202798\n",
      "Episode * 1040 * Avg Steps 269.11 * Episodic Reward is ==> -92.34846875267986 * Lastest 100 Episods Avg Reward is ==> -29.388061340341945\n",
      "Episode * 1050 * Avg Steps 266.82 * Episodic Reward is ==> 74.9094455394791 * Lastest 100 Episods Avg Reward is ==> -33.93096371748681\n",
      "Episode * 1060 * Avg Steps 265.83 * Episodic Reward is ==> 109.15912175075692 * Lastest 100 Episods Avg Reward is ==> -35.33160585649803\n",
      "Episode * 1070 * Avg Steps 265.83 * Episodic Reward is ==> 115.7045096553318 * Lastest 100 Episods Avg Reward is ==> -35.62764921066891\n",
      "Episode * 1080 * Avg Steps 265.37 * Episodic Reward is ==> 134.85430805254376 * Lastest 100 Episods Avg Reward is ==> -31.983351434610583\n",
      "Episode * 1090 * Avg Steps 267.62 * Episodic Reward is ==> 102.89894689540573 * Lastest 100 Episods Avg Reward is ==> -13.686119960418079\n",
      "Episode * 1100 * Avg Steps 273.46 * Episodic Reward is ==> 33.470846852628505 * Lastest 100 Episods Avg Reward is ==> 3.16973755476589\n",
      "Episode * 1110 * Avg Steps 283.06 * Episodic Reward is ==> 178.1936960237719 * Lastest 100 Episods Avg Reward is ==> 26.00002982606432\n",
      "Episode * 1120 * Avg Steps 288.23 * Episodic Reward is ==> 160.08926180579067 * Lastest 100 Episods Avg Reward is ==> 47.22414698946567\n",
      "Episode * 1130 * Avg Steps 291.26 * Episodic Reward is ==> 49.382222229059224 * Lastest 100 Episods Avg Reward is ==> 63.94546043755884\n",
      "Episode * 1140 * Avg Steps 293.24 * Episodic Reward is ==> 58.45211365242819 * Lastest 100 Episods Avg Reward is ==> 70.55355970923479\n",
      "Episode * 1150 * Avg Steps 295.53 * Episodic Reward is ==> 73.67754261067596 * Lastest 100 Episods Avg Reward is ==> 81.82224739291773\n",
      "Episode * 1160 * Avg Steps 295.3 * Episodic Reward is ==> 152.83862668885152 * Lastest 100 Episods Avg Reward is ==> 88.4852573369139\n",
      "Episode * 1170 * Avg Steps 295.3 * Episodic Reward is ==> 135.0759485616047 * Lastest 100 Episods Avg Reward is ==> 93.20469217483793\n",
      "Episode * 1180 * Avg Steps 295.11 * Episodic Reward is ==> 140.94776305002404 * Lastest 100 Episods Avg Reward is ==> 103.56697242996883\n",
      "Episode * 1190 * Avg Steps 294.27 * Episodic Reward is ==> 100.5502679672018 * Lastest 100 Episods Avg Reward is ==> 102.40988259214524\n",
      "Episode * 1200 * Avg Steps 290.89 * Episodic Reward is ==> -4.297664496682472 * Lastest 100 Episods Avg Reward is ==> 98.90114762303095\n",
      "Episode * 1210 * Avg Steps 289.57 * Episodic Reward is ==> 83.3444378857865 * Lastest 100 Episods Avg Reward is ==> 92.56945602141418\n",
      "Episode * 1220 * Avg Steps 289.77 * Episodic Reward is ==> 104.00298148282639 * Lastest 100 Episods Avg Reward is ==> 90.53398797101715\n",
      "Episode * 1230 * Avg Steps 291.3 * Episodic Reward is ==> 135.93699903653817 * Lastest 100 Episods Avg Reward is ==> 90.209327122194\n",
      "Episode * 1240 * Avg Steps 291.45 * Episodic Reward is ==> 72.46028942396455 * Lastest 100 Episods Avg Reward is ==> 96.92590560557073\n",
      "Episode * 1250 * Avg Steps 291.45 * Episodic Reward is ==> 77.43201496922023 * Lastest 100 Episods Avg Reward is ==> 98.8742399522203\n",
      "Episode * 1260 * Avg Steps 292.67 * Episodic Reward is ==> 47.176114188198895 * Lastest 100 Episods Avg Reward is ==> 96.65063256906573\n",
      "Episode * 1270 * Avg Steps 292.67 * Episodic Reward is ==> 86.61673314217373 * Lastest 100 Episods Avg Reward is ==> 92.71717418576696\n",
      "Episode * 1280 * Avg Steps 293.32 * Episodic Reward is ==> 55.16910613096766 * Lastest 100 Episods Avg Reward is ==> 81.88966637738609\n",
      "Episode * 1290 * Avg Steps 294.16 * Episodic Reward is ==> 51.43734171616986 * Lastest 100 Episods Avg Reward is ==> 80.66840158696854\n",
      "Episode * 1300 * Avg Steps 297.54 * Episodic Reward is ==> 103.43480506295 * Lastest 100 Episods Avg Reward is ==> 85.20397962694419\n",
      "Episode * 1310 * Avg Steps 298.87 * Episodic Reward is ==> 104.8411567679936 * Lastest 100 Episods Avg Reward is ==> 90.09230227531836\n",
      "Episode * 1320 * Avg Steps 298.87 * Episodic Reward is ==> 126.38359662314113 * Lastest 100 Episods Avg Reward is ==> 91.17427477829274\n",
      "Episode * 1330 * Avg Steps 300.0 * Episodic Reward is ==> 126.93670795572825 * Lastest 100 Episods Avg Reward is ==> 91.87922671126361\n",
      "Episode * 1340 * Avg Steps 300.0 * Episodic Reward is ==> 79.62816105279575 * Lastest 100 Episods Avg Reward is ==> 91.70429503954719\n",
      "Episode * 1350 * Avg Steps 300.0 * Episodic Reward is ==> 82.32641711132906 * Lastest 100 Episods Avg Reward is ==> 90.49322065819234\n",
      "Episode * 1360 * Avg Steps 300.0 * Episodic Reward is ==> 138.1266879560026 * Lastest 100 Episods Avg Reward is ==> 92.67577911709012\n",
      "Episode * 1370 * Avg Steps 300.0 * Episodic Reward is ==> 104.27085208528275 * Lastest 100 Episods Avg Reward is ==> 93.80214835778008\n",
      "Episode * 1380 * Avg Steps 300.0 * Episodic Reward is ==> 115.33389994765103 * Lastest 100 Episods Avg Reward is ==> 100.46640687042101\n",
      "Episode * 1390 * Avg Steps 300.0 * Episodic Reward is ==> 64.91130854306947 * Lastest 100 Episods Avg Reward is ==> 102.40805412691003\n",
      "Episode * 1400 * Avg Steps 300.0 * Episodic Reward is ==> 91.73490207635851 * Lastest 100 Episods Avg Reward is ==> 100.67622332985974\n",
      "Episode * 1410 * Avg Steps 300.0 * Episodic Reward is ==> 67.4128758678992 * Lastest 100 Episods Avg Reward is ==> 95.34393470200567\n",
      "Episode * 1420 * Avg Steps 298.99 * Episodic Reward is ==> 44.960081159314 * Lastest 100 Episods Avg Reward is ==> 86.04974003314508\n",
      "Episode * 1430 * Avg Steps 298.35 * Episodic Reward is ==> -18.081607984953838 * Lastest 100 Episods Avg Reward is ==> 75.75166698245268\n",
      "Episode * 1440 * Avg Steps 297.67 * Episodic Reward is ==> 5.18950983909592 * Lastest 100 Episods Avg Reward is ==> 66.34465718868661\n",
      "Episode * 1450 * Avg Steps 297.06 * Episodic Reward is ==> 114.94688379628712 * Lastest 100 Episods Avg Reward is ==> 61.00952098065176\n",
      "Episode * 1460 * Avg Steps 297.06 * Episodic Reward is ==> 148.0224204159729 * Lastest 100 Episods Avg Reward is ==> 59.78281283878311\n",
      "Episode * 1470 * Avg Steps 297.06 * Episodic Reward is ==> 115.77068908493277 * Lastest 100 Episods Avg Reward is ==> 60.847258839512016\n",
      "Episode * 1480 * Avg Steps 297.06 * Episodic Reward is ==> 76.35152093769224 * Lastest 100 Episods Avg Reward is ==> 59.75913160722784\n",
      "Episode * 1490 * Avg Steps 297.06 * Episodic Reward is ==> 109.35607069023354 * Lastest 100 Episods Avg Reward is ==> 62.60488480034519\n",
      "Episode * 1500 * Avg Steps 297.06 * Episodic Reward is ==> 101.05503428241578 * Lastest 100 Episods Avg Reward is ==> 65.86708959250907\n",
      "Episode * 1510 * Avg Steps 297.06 * Episodic Reward is ==> 112.27991161822898 * Lastest 100 Episods Avg Reward is ==> 72.29539179945117\n",
      "Episode * 1520 * Avg Steps 298.07 * Episodic Reward is ==> 76.9818831865767 * Lastest 100 Episods Avg Reward is ==> 81.29647955632417\n",
      "Episode * 1530 * Avg Steps 298.71 * Episodic Reward is ==> 108.47625166961919 * Lastest 100 Episods Avg Reward is ==> 88.85451252374754\n",
      "Episode * 1540 * Avg Steps 299.39 * Episodic Reward is ==> 125.43955037629337 * Lastest 100 Episods Avg Reward is ==> 96.70021684105949\n",
      "Episode * 1550 * Avg Steps 299.28 * Episodic Reward is ==> 82.81236527901532 * Lastest 100 Episods Avg Reward is ==> 91.91153040281563\n",
      "Episode * 1560 * Avg Steps 299.28 * Episodic Reward is ==> 51.290410270669966 * Lastest 100 Episods Avg Reward is ==> 92.11012303153589\n",
      "Episode * 1570 * Avg Steps 299.28 * Episodic Reward is ==> 105.71806735387375 * Lastest 100 Episods Avg Reward is ==> 89.69265147601689\n",
      "Episode * 1580 * Avg Steps 299.28 * Episodic Reward is ==> 152.43567861686387 * Lastest 100 Episods Avg Reward is ==> 89.97694303558687\n",
      "Episode * 1590 * Avg Steps 299.28 * Episodic Reward is ==> 117.09178037480709 * Lastest 100 Episods Avg Reward is ==> 88.29028295002647\n",
      "Episode * 1600 * Avg Steps 299.28 * Episodic Reward is ==> 90.39075524442448 * Lastest 100 Episods Avg Reward is ==> 87.43908174832386\n",
      "Episode * 1610 * Avg Steps 299.28 * Episodic Reward is ==> 142.56132767093956 * Lastest 100 Episods Avg Reward is ==> 87.15041339695271\n",
      "Episode * 1620 * Avg Steps 299.28 * Episodic Reward is ==> 114.00880146871332 * Lastest 100 Episods Avg Reward is ==> 88.98026642141427\n",
      "Episode * 1630 * Avg Steps 299.28 * Episodic Reward is ==> 128.2220167530255 * Lastest 100 Episods Avg Reward is ==> 90.29174770479223\n",
      "Episode * 1640 * Avg Steps 299.28 * Episodic Reward is ==> 88.83986646433475 * Lastest 100 Episods Avg Reward is ==> 88.79317061560744\n",
      "Episode * 1650 * Avg Steps 300.0 * Episodic Reward is ==> 104.7804170542801 * Lastest 100 Episods Avg Reward is ==> 96.686991276157\n",
      "Episode * 1660 * Avg Steps 300.0 * Episodic Reward is ==> 112.92884437444684 * Lastest 100 Episods Avg Reward is ==> 95.2536215697483\n",
      "Episode * 1670 * Avg Steps 300.0 * Episodic Reward is ==> 122.15915615556112 * Lastest 100 Episods Avg Reward is ==> 96.09600026882964\n",
      "Episode * 1680 * Avg Steps 300.0 * Episodic Reward is ==> 66.37408567779373 * Lastest 100 Episods Avg Reward is ==> 95.0141813524416\n",
      "Episode * 1690 * Avg Steps 300.0 * Episodic Reward is ==> 61.75071455691176 * Lastest 100 Episods Avg Reward is ==> 93.39330049179497\n",
      "Episode * 1700 * Avg Steps 300.0 * Episodic Reward is ==> 60.16935327593532 * Lastest 100 Episods Avg Reward is ==> 91.36810122531064\n",
      "Episode * 1710 * Avg Steps 300.0 * Episodic Reward is ==> 86.07628209260841 * Lastest 100 Episods Avg Reward is ==> 90.58761935024573\n",
      "Episode * 1720 * Avg Steps 300.0 * Episodic Reward is ==> 112.27784516631466 * Lastest 100 Episods Avg Reward is ==> 88.07588167532177\n",
      "Episode * 1730 * Avg Steps 300.0 * Episodic Reward is ==> 124.07327512526848 * Lastest 100 Episods Avg Reward is ==> 89.84363220230063\n",
      "Episode * 1740 * Avg Steps 300.0 * Episodic Reward is ==> 108.92239644239434 * Lastest 100 Episods Avg Reward is ==> 92.20280388923193\n",
      "Episode * 1750 * Avg Steps 300.0 * Episodic Reward is ==> 92.80267924851948 * Lastest 100 Episods Avg Reward is ==> 94.66989779815802\n",
      "Episode * 1760 * Avg Steps 300.0 * Episodic Reward is ==> 108.647553926296 * Lastest 100 Episods Avg Reward is ==> 97.13850171076356\n",
      "Episode * 1770 * Avg Steps 300.0 * Episodic Reward is ==> 126.81901725924627 * Lastest 100 Episods Avg Reward is ==> 99.77777982997186\n",
      "Episode * 1780 * Avg Steps 300.0 * Episodic Reward is ==> 86.42490719191589 * Lastest 100 Episods Avg Reward is ==> 102.25960793415133\n",
      "Episode * 1790 * Avg Steps 300.0 * Episodic Reward is ==> 130.791107804685 * Lastest 100 Episods Avg Reward is ==> 105.57149544067107\n",
      "Episode * 1800 * Avg Steps 297.98 * Episodic Reward is ==> 35.23513555591521 * Lastest 100 Episods Avg Reward is ==> 107.27412482026739\n",
      "Episode * 1810 * Avg Steps 295.42 * Episodic Reward is ==> 83.95687737984477 * Lastest 100 Episods Avg Reward is ==> 111.65064026121647\n",
      "Episode * 1820 * Avg Steps 285.01 * Episodic Reward is ==> 46.64196595589951 * Lastest 100 Episods Avg Reward is ==> 115.05784901816514\n",
      "Episode * 1830 * Avg Steps 269.78 * Episodic Reward is ==> 270.31020032607114 * Lastest 100 Episods Avg Reward is ==> 112.61504512230968\n",
      "Episode * 1840 * Avg Steps 253.19 * Episodic Reward is ==> 3.304165553121919 * Lastest 100 Episods Avg Reward is ==> 114.0622786248072\n",
      "Episode * 1850 * Avg Steps 235.18 * Episodic Reward is ==> -28.998206685809606 * Lastest 100 Episods Avg Reward is ==> 105.94186002666397\n",
      "Episode * 1860 * Avg Steps 217.14 * Episodic Reward is ==> 265.11308624589606 * Lastest 100 Episods Avg Reward is ==> 100.0035409131944\n",
      "Episode * 1870 * Avg Steps 196.47 * Episodic Reward is ==> -20.09937692291308 * Lastest 100 Episods Avg Reward is ==> 84.68031746088418\n",
      "Episode * 1880 * Avg Steps 180.27 * Episodic Reward is ==> -30.029467740142806 * Lastest 100 Episods Avg Reward is ==> 79.12721021476278\n",
      "Episode * 1890 * Avg Steps 167.72 * Episodic Reward is ==> -15.001042315357324 * Lastest 100 Episods Avg Reward is ==> 80.33748863002266\n",
      "Episode * 1900 * Avg Steps 159.69 * Episodic Reward is ==> 136.8737436738473 * Lastest 100 Episods Avg Reward is ==> 84.2064483436591\n",
      "Episode * 1910 * Avg Steps 151.77 * Episodic Reward is ==> 115.6394037116522 * Lastest 100 Episods Avg Reward is ==> 75.90157976802351\n",
      "Episode * 1920 * Avg Steps 147.09 * Episodic Reward is ==> 228.73545334518445 * Lastest 100 Episods Avg Reward is ==> 67.74568494537202\n",
      "Episode * 1930 * Avg Steps 144.61 * Episodic Reward is ==> 268.07050042384964 * Lastest 100 Episods Avg Reward is ==> 61.07067783711965\n",
      "Episode * 1940 * Avg Steps 144.26 * Episodic Reward is ==> -63.41803651795895 * Lastest 100 Episods Avg Reward is ==> 50.52639737708623\n",
      "Episode * 1950 * Avg Steps 147.51 * Episodic Reward is ==> 23.85451207979355 * Lastest 100 Episods Avg Reward is ==> 49.78862278891786\n",
      "Episode * 1960 * Avg Steps 151.07 * Episodic Reward is ==> 33.447512125495166 * Lastest 100 Episods Avg Reward is ==> 51.20244126315567\n",
      "Episode * 1970 * Avg Steps 154.94 * Episodic Reward is ==> 3.8821964791289645 * Lastest 100 Episods Avg Reward is ==> 59.457584625302744\n",
      "Episode * 1980 * Avg Steps 159.74 * Episodic Reward is ==> 50.50308864573685 * Lastest 100 Episods Avg Reward is ==> 67.97465434826559\n",
      "Episode * 1990 * Avg Steps 160.34 * Episodic Reward is ==> 283.253580184577 * Lastest 100 Episods Avg Reward is ==> 64.90017619808869\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_f0d0a3b7-61a0-44eb-8aea-3f5c357b1271\", \"ddpg_per_False_20210405_183122.csv\", 92162)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddpg.train(max_episodes=2000,max_steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "KuF4rIYqwyZ0",
    "outputId": "82d3254f-38f4-4036-d025-b0ec836998ff"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_2c6ea1ca-8946-4d87-823f-45389a857d93\", \"new_ddpg_baseversion_withoutnoise_per_actor_final_ep.h5\", 563712)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_019a47a2-411d-44d2-81e3-2494c08eee94\", \"new_ddpg_baseversion_withoutnoise_per_critic_final_ep.h5\", 767808)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddpg.actor_model.save(\"new_ddpg_baseversion_withoutnoise_per_actor_final_ep.h5\")\n",
    "ddpg.critic_model.save(\"new_ddpg_baseversion_withoutnoise_per_critic_final_ep.h5\")\n",
    "files.download(\"new_ddpg_baseversion_withoutnoise_per_actor_final_ep.h5\")\n",
    "files.download(\"new_ddpg_baseversion_withoutnoise_per_critic_final_ep.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYhtvRUQY_Ov"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ddpg_per_new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
